{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# zlib License\n",
    "#\n",
    "# (C) 2023 Murtaza Safdari <musafdar@cern.ch>, Jongho Lee <jongho.lee@cern.ch>, Cristovao Beirao da Cruz e Sliva <cberiaod@cern.ch>\n",
    "#\n",
    "# This software is provided 'as-is', without any express or implied\n",
    "# warranty.  In no event will the authors be held liable for any damages\n",
    "# arising from the use of this software.\n",
    "#\n",
    "# Permission is granted to anyone to use this software for any purpose,\n",
    "# including commercial applications, and to alter it and redistribute it\n",
    "# freely, subject to the following restrictions:\n",
    "#\n",
    "# 1. The origin of this software must not be misrepresented; you must not\n",
    "#    claim that you wrote the original software. If you use this software\n",
    "#    in a product, an acknowledgment in the product documentation would be\n",
    "#    appreciated but is not required.\n",
    "# 2. Altered source versions must be plainly marked as such, and must not be\n",
    "#    misrepresented as being the original software.\n",
    "# 3. This notice may not be removed or altered from any source distribution.\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beamtest_analysis_helper as helper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'run12'\n",
    "original_files = glob(f'./desy_TB_{run_name}/*feather')\n",
    "\n",
    "board_ids_to_analyze = [0, 1, 3]\n",
    "board_ids_to_drop = [2]\n",
    "columns_to_read = ['evt', 'board', 'row', 'col', 'toa', 'tot', 'cal']\n",
    "dut_id = 1\n",
    "ref_id = 3\n",
    "\n",
    "# columns_want_to_drop = [f'toa_{i}' for i in set(list_of_all_boards)-set(list_of_ignore_boards)]\n",
    "\n",
    "columns_want_to_group = []\n",
    "for i in board_ids_to_analyze:\n",
    "    columns_want_to_group.append(f'row_{i}')\n",
    "    columns_want_to_group.append(f'col_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for isampling in tqdm(range(10)):\n",
    "    files = random.sample(original_files, k=int(0.1*len(original_files)))\n",
    "\n",
    "    last_evt = 0\n",
    "    dataframes = []\n",
    "\n",
    "    for idx, ifile in enumerate(files):\n",
    "        tmp_df = pd.read_feather(ifile, columns=columns_to_read)\n",
    "\n",
    "        if idx > 0:\n",
    "            tmp_df['evt'] += last_evt\n",
    "        last_evt += tmp_df['evt'].unique()[-1]\n",
    "\n",
    "        ## Selecting good hits\n",
    "        tdc_cuts = {}\n",
    "        for idx in board_ids_to_analyze:\n",
    "            # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "            if idx == 0:\n",
    "                tdc_cuts[idx] = [tmp_df.loc[tmp_df['board'] == idx]['cal'].mode()[0]-50, tmp_df.loc[tmp_df['board'] == idx]['cal'].mode()[0]+50,  100, 500, 0, 600]\n",
    "            else:\n",
    "                tdc_cuts[idx] = [tmp_df.loc[tmp_df['board'] == idx]['cal'].mode()[0]-50, tmp_df.loc[tmp_df['board'] == idx]['cal'].mode()[0]+50,  0, 1100, 0, 600]\n",
    "\n",
    "        filtered_df = helper.tdc_event_selection(tmp_df, tdc_cuts_dict=tdc_cuts)\n",
    "        del tmp_df\n",
    "\n",
    "        if filtered_df.empty:\n",
    "            continue\n",
    "\n",
    "        event_board_counts = filtered_df.groupby(['evt', 'board']).size().unstack(fill_value=0)\n",
    "        event_selection_col = None\n",
    "\n",
    "        trig_selection = (event_board_counts[0] == 1)\n",
    "        ref_selection = (event_board_counts[ref_id] == 1)\n",
    "        event_selection_col = trig_selection & ref_selection\n",
    "\n",
    "        selected_subset_df = filtered_df[filtered_df['evt'].isin(event_board_counts[event_selection_col].index)]\n",
    "        selected_subset_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        dataframes.append(selected_subset_df)\n",
    "        del event_board_counts, selected_subset_df, event_selection_col\n",
    "\n",
    "    df = pd.concat(dataframes)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    del dataframes\n",
    "\n",
    "    single_filtered_df = helper.singlehit_event_clear(df, ignore_boards=board_ids_to_drop)\n",
    "    pivot_data_df = helper.making_pivot(single_filtered_df, 'evt', 'board', set({'board', 'evt', 'cal', 'tot'}), ignore_boards=board_ids_to_drop)\n",
    "    del single_filtered_df\n",
    "\n",
    "    min_hit_counter = 500*len(files)/len(original_files)\n",
    "    # print('# of track cut:', min_hit_counter)\n",
    "    combinations_df = pivot_data_df.groupby(columns_want_to_group).count()\n",
    "    combinations_df['count'] = combinations_df['toa_0']\n",
    "    # combinations_df.drop(columns_want_to_drop, axis=1, inplace=True)\n",
    "    track_df = combinations_df.loc[combinations_df['count'] > min_hit_counter]\n",
    "    track_df.reset_index(inplace=True)\n",
    "    del pivot_data_df, combinations_df\n",
    "\n",
    "    row_delta_TR = np.abs(track_df['row_0'] - track_df[f'row_{ref_id}']) <= 2\n",
    "    row_delta_TD = np.abs(track_df['row_0'] - track_df[f'row_{dut_id}']) <= 1\n",
    "    col_delta_TR = np.abs(track_df['col_0'] - track_df[f'col_{ref_id}']) <= 2\n",
    "    col_delta_TD = np.abs(track_df['col_0'] - track_df[f'col_{dut_id}']) <= 1\n",
    "\n",
    "    track_condition = (row_delta_TR) & (col_delta_TR) & (row_delta_TD) & (col_delta_TD)\n",
    "    track_df = track_df[track_condition]\n",
    "\n",
    "    final_list.append(track_df)\n",
    "    del track_condition, track_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(final_list)\n",
    "final_df.drop(columns=['count'], inplace=True)\n",
    "final_df = final_df.drop_duplicates(subset=columns_want_to_group, keep='first')\n",
    "final_df.to_csv(f'{run_name}_good_track_candidates.csv', index=False)\n",
    "\n",
    "del final_list, final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
