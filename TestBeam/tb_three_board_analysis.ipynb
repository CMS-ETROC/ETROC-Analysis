{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# zlib License\n",
    "#\n",
    "# (C) 2023 Murtaza Safdari <musafdar@cern.ch>, Jongho Lee <jongho.lee@cern.ch>, Cristovao Beirao da Cruz e Sliva <cberiaod@cern.ch>\n",
    "#\n",
    "# This software is provided 'as-is', without any express or implied\n",
    "# warranty.  In no event will the authors be held liable for any damages\n",
    "# arising from the use of this software.\n",
    "#\n",
    "# Permission is granted to anyone to use this software for any purpose,\n",
    "# including commercial applications, and to alter it and redistribute it\n",
    "# freely, subject to the following restrictions:\n",
    "#\n",
    "# 1. The origin of this software must not be misrepresented; you must not\n",
    "#    claim that you wrote the original software. If you use this software\n",
    "#    in a product, an acknowledgment in the product documentation would be\n",
    "#    appreciated but is not required.\n",
    "# 2. Altered source versions must be plainly marked as such, and must not be\n",
    "#    misrepresented as being the original software.\n",
    "# 3. This notice may not be removed or altered from any source distribution.\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beamtest_analysis_helper as helper\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "hep.style.use('CMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!\n",
    "# It is very important to correctly set the chip name, this value is stored with the data\n",
    "\n",
    "today = datetime.date.today().isoformat()\n",
    "fig_outdir = Path('../../ETROC-figures')\n",
    "fig_outdir = fig_outdir / (today + '_Array_Test_Results')\n",
    "fig_outdir.mkdir(exist_ok=True)\n",
    "fig_path = str(fig_outdir)\n",
    "\n",
    "\n",
    "chip_labels = [0, 1, 2, 3]\n",
    "chip_names = [\"ET2_EPIR_Pair1\", \"ET2_BAR_4\", \"ET2_BAR_6\", \"ET2_CNM_1-3\"]\n",
    "offsets = [15, 6, 15, 6]\n",
    "high_voltages = [260, 260, 260, 200]\n",
    "\n",
    "chip_fignames = chip_names\n",
    "chip_figtitles = [\n",
    "    f\"(Trigger) Pair1 HV{high_voltages[0]}V OS:{offsets[0]}\",\n",
    "    f\"(DUT1) Bar4 HV{high_voltages[1]}V OS:{offsets[1]}\",\n",
    "    f\"(Reference) Bar6 HV{high_voltages[2]}V OS:{offsets[2]}\",\n",
    "    f\"(DUT2) CNM (HPK) 1-3 HV{high_voltages[3]}V OS:{offsets[3]}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "\n",
    "fig = plt.figure(dpi=50, figsize=(5,5))\n",
    "gs = fig.add_gridspec(1,1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pandas dataframe and filter events with a single hit per board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('./desy_TB_run12/Run_12_loop_[1-9].feather')\n",
    "\n",
    "last_evt = 0\n",
    "dataframes = []\n",
    "\n",
    "for idx, ifile in enumerate(files):\n",
    "    tmp_df = pd.read_feather(ifile)\n",
    "    tmp_df.drop(columns=['evt_number', 'bcid', 'l1a_counter', 'ea'], inplace=True)\n",
    "\n",
    "    if idx > 0:\n",
    "        tmp_df['evt'] += last_evt\n",
    "    last_evt += tmp_df['evt'].unique()[-1]\n",
    "\n",
    "\n",
    "    ## Selecting good hits\n",
    "    tdc_cuts = {\n",
    "        # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "        0: [tmp_df.loc[tmp_df['board'] == 0]['cal'].mode()[0]-50, tmp_df.loc[tmp_df['board'] == 0]['cal'].mode()[0]+50, 100, 500,  0, 600],\n",
    "        1: [tmp_df.loc[tmp_df['board'] == 1]['cal'].mode()[0]-50, tmp_df.loc[tmp_df['board'] == 1]['cal'].mode()[0]+50,   0, 1100, 0, 600],\n",
    "        2: [tmp_df.loc[tmp_df['board'] == 2]['cal'].mode()[0]-50, tmp_df.loc[tmp_df['board'] == 2]['cal'].mode()[0]+50,   0, 1100, 0, 600],\n",
    "        3: [tmp_df.loc[tmp_df['board'] == 3]['cal'].mode()[0]-50, tmp_df.loc[tmp_df['board'] == 3]['cal'].mode()[0]+50,   0, 1100, 0, 600],\n",
    "    }\n",
    "\n",
    "    filtered_df = helper.tdc_event_selection(tmp_df, tdc_cuts_dict=tdc_cuts)\n",
    "    del tmp_df\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        continue\n",
    "\n",
    "    event_board_counts = filtered_df.groupby(['evt', 'board']).size().unstack(fill_value=0)\n",
    "    event_selection_col = None\n",
    "\n",
    "    trig_selection = (event_board_counts[0] == 1)\n",
    "    ref_selection = (event_board_counts[2] == 1)\n",
    "    event_selection_col = trig_selection & ref_selection\n",
    "\n",
    "    selected_event_numbers = event_board_counts[event_selection_col].index\n",
    "    selected_subset_df = filtered_df[filtered_df['evt'].isin(selected_event_numbers)]\n",
    "    selected_subset_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    dataframes.append(selected_subset_df)\n",
    "    del event_board_counts, selected_event_numbers, selected_subset_df, event_selection_col\n",
    "\n",
    "df = pd.concat(dataframes)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "del dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_inclusive = helper.return_hist(df, chip_names, chip_labels, hist_bins=[100, 128, 128])\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[3], chip_fignames[3], chip_figtitles[3], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "del h_inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_heatmap_byPandas(df, chipLabels=chip_labels, figtitle=chip_figtitles, figtitle_tag='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a one pixel of a trigger board, and make occupancy map of other boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_df = helper.efficiency_with_single_board(df, pixel=(7, 5), board_id=3)\n",
    "helper.plot_heatmap_byPandas(eff_df, chip_labels, chip_figtitles, \"inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_df = helper.efficiency_with_two_boards(df, pixel=(7, 5), board_ids=(0, 2))\n",
    "helper.plot_heatmap_byPandas(eff_df, chip_labels, chip_figtitles, \"inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_inclusive = helper.return_hist(eff_df, chip_names, chip_labels, hist_bins=[100, 256, 512])\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_inclusive, chip_names[3], chip_fignames[3], chip_figtitles[3], fig_path, save=False, show=True,\n",
    "                                tag=\"inclusive\", title_tag=\", inclusive\", slide_friendly=True)\n",
    "\n",
    "del h_inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event counter based on possible track combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_filtered_df = helper.singlehit_event_clear(df)\n",
    "pivot_data_df = helper.making_pivot(single_filtered_df, 'evt', 'board', set({'board', 'evt', 'cal', 'tot'}), ignore_boards=[3])\n",
    "# pivot_data_df = helper.making_pivot(df, 'evt', 'board', set({'board', 'evt', 'cal', 'tot'}), ignore_boards=[2])\n",
    "del single_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hit_counter = 1200\n",
    "# combinations_df = pivot_data_df.groupby(['row_0', 'col_0', 'row_1', 'col_1', 'row_2', 'col_2', 'row_3', 'col_3', ]).count()\n",
    "combinations_df = pivot_data_df.groupby(['row_0', 'col_0', 'row_1', 'col_1', 'row_2', 'col_2', ]).count()\n",
    "combinations_df['count'] = combinations_df['toa_0']\n",
    "# combinations_df.drop(['toa_0', 'toa_1', 'toa_2', 'toa_3'], axis=1, inplace=True)\n",
    "combinations_df.drop(['toa_0', 'toa_1', 'toa_2'], axis=1, inplace=True)\n",
    "track_df = combinations_df.loc[combinations_df['count'] > min_hit_counter]\n",
    "track_df.reset_index(inplace=True)\n",
    "del combinations_df\n",
    "# del pivot_data_df, combinations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (0.5*(track_df['row_0'] + track_df['row_2']) - track_df['row_1'])**2\n",
    "b = (0.5*(track_df['col_0'] + track_df['col_2']) - track_df['col_1'])**2\n",
    "track_selection = (np.sqrt(a+b) <= 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df[track_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------\n",
    "def new_pixel_filter(\n",
    "        input_df: pd.DataFrame,\n",
    "        pixel_dict: dict\n",
    "    ):\n",
    "    # Create boolean masks for each board's filtering criteria\n",
    "    masks = {}\n",
    "    for board, pix in pixel_dict.items():\n",
    "        mask = (\n",
    "            (input_df['board'] == board) & (input_df['row'] == pix[0]) & (input_df['col'] == pix[1])\n",
    "        )\n",
    "        masks[board] = mask\n",
    "\n",
    "    # Combine the masks using logical All\n",
    "    combined_mask = pd.concat(masks, axis=1).any(axis=1)\n",
    "\n",
    "    # Apply the combined mask to the DataFrame\n",
    "    filtered = input_df[combined_mask].reset_index(drop=True)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (df['board'] == 0) & (df['row'] == 0) & (df['col'] == 4)\n",
    "mask2 = (df['board'] == 1) & (df['row'] == 0) & (df['col'] == 4)\n",
    "mask3 = (df['board'] == 2) & (df['row'] == 0) & (df['col'] == 4)\n",
    "\n",
    "masks = {\n",
    "    0: mask1,\n",
    "    1: mask2,\n",
    "    2: mask3,\n",
    "}\n",
    "\n",
    "combined_mask = pd.concat(masks, axis=1)\n",
    "# combined_mask[(combined_mask[1] == True)]\n",
    "a = (combined_mask[0] == True) | (combined_mask[1] == True) | (combined_mask[2] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_dict = {\n",
    "    # board ID: [row, col]\n",
    "    0: [ 0, 4],\n",
    "    1: [ 0, 4],\n",
    "    2: [ 0, 4],\n",
    "}\n",
    "\n",
    "filtered_group = new_pixel_filter(df, pix_dict)\n",
    "filtered_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_to_analyze = [0, 1, 2]\n",
    "track_pivots = []\n",
    "\n",
    "for i in range(len(track_df[track_selection])):\n",
    "\n",
    "    pix_dict = {}\n",
    "    for idx in board_to_analyze:\n",
    "        pix_dict[idx] = [track_df.iloc[i][f'row_{idx}'], track_df.iloc[i][f'col_{idx}']]\n",
    "\n",
    "    filtered_group = helper.pixel_filter(df, pix_dict)\n",
    "    print(filtered_group)\n",
    "\n",
    "    # pivot_table = filtered_group.pivot(index=[\"evt\"], columns=[\"board\"], values=[\"row\", \"col\", \"toa\", \"tot\", \"cal\"])\n",
    "    # track_pivots.append(pivot_table)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_pivots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_dict = {\n",
    "    # board ID: [row, col]\n",
    "    0: [ 9, 5],\n",
    "    1: [ 9, 5],\n",
    "    2: [ 9, 5],\n",
    "    3: [ -1, -1],\n",
    "}\n",
    "\n",
    "filtered_group = helper.pixel_filter(df, pix_dict)\n",
    "filtered_group.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap algorithm to calculate time resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_pivots = []\n",
    "\n",
    "for i in range(len(track_df)):\n",
    "\n",
    "    pix_dict = {}\n",
    "    for idx in board_to_analyze:\n",
    "        pix_dict[idx] = [track_df.iloc[i][f'row_{idx}'], track_df.iloc[i][f'col_{idx}']]\n",
    "\n",
    "    filtered_group = helper.pixel_filter(df, pix_dict)\n",
    "    filtered_group = helper.singlehit_event_clear_func(filtered_group)\n",
    "    pivot_table = filtered_group.pivot(index=[\"evt\"], columns=[\"board\"], values=[\"row\", \"col\", \"toa\", \"tot\", \"cal\"])\n",
    "    track_pivots.append(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_pivots[0]['row'][0].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "final_dict = {}\n",
    "\n",
    "for idx in board_to_analyze:\n",
    "    final_dict[f'row{idx}'] = []\n",
    "    final_dict[f'col{idx}'] = []\n",
    "    final_dict[f'res{idx}'] = []\n",
    "    final_dict[f'err{idx}'] = []\n",
    "\n",
    "for itable in tqdm(track_pivots):\n",
    "\n",
    "    sum_arr = {}\n",
    "    sum_square_arr = {}\n",
    "    iteration = 100\n",
    "    sampling_fraction = 0.75\n",
    "    counter = 0\n",
    "\n",
    "    for idx in board_to_analyze:\n",
    "        sum_arr[idx] = 0\n",
    "        sum_square_arr[idx] = 0\n",
    "\n",
    "    for iloop in range(iteration):\n",
    "\n",
    "        try_df = itable.reset_index()\n",
    "        tdc_cuts = {}\n",
    "        for idx in board_to_analyze:\n",
    "            # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "            if idx == 0:\n",
    "                tdc_cuts[idx] = [try_df['cal'][idx].mean()-5, try_df['cal'][idx].mean()+5,  350, 500, 0, 600]\n",
    "            else:\n",
    "                tdc_cuts[idx] = [try_df['cal'][idx].mean()-5, try_df['cal'][idx].mean()+5,  0, 1100, 0, 600]\n",
    "\n",
    "        tdc_filtered_df = helper.tdc_event_selection_pivot(try_df, tdc_cuts)\n",
    "        del try_df, tdc_cuts\n",
    "\n",
    "        n = int(sampling_fraction*tdc_filtered_df.shape[0])\n",
    "        indices = np.random.choice(tdc_filtered_df['evt'].unique(), n, replace=False)\n",
    "        tdc_filtered_df = tdc_filtered_df.loc[tdc_filtered_df['evt'].isin(indices)]\n",
    "\n",
    "        if tdc_filtered_df.shape[0] < iteration/(3.*(1-sampling_fraction)):\n",
    "            print('Warning!! Sampling size is too small. Skipping this track')\n",
    "            break\n",
    "\n",
    "        d = {\n",
    "            'evt': tdc_filtered_df['evt'].unique(),\n",
    "        }\n",
    "\n",
    "        for idx in board_to_analyze:\n",
    "            bins = 3.125/tdc_filtered_df['cal'][idx].mean()\n",
    "            d[f'toa_b{str(idx)}'] = 12.5 - tdc_filtered_df['toa'][idx] * bins\n",
    "            d[f'tot_b{str(idx)}'] = (2*tdc_filtered_df['tot'][idx] - np.floor(tdc_filtered_df['tot'][idx]/32)) * bins\n",
    "\n",
    "        df_in_time = pd.DataFrame(data=d)\n",
    "        del d, tdc_filtered_df\n",
    "\n",
    "        corr_toas = helper.three_board_iterative_timewalk_correction(df_in_time, 5, 3, board_list=board_to_analyze)\n",
    "\n",
    "        diffs = {}\n",
    "        for board_a in board_to_analyze:\n",
    "            for board_b in board_to_analyze:\n",
    "                if board_b <= board_a:\n",
    "                    continue\n",
    "                name = f\"{board_a}{board_b}\"\n",
    "                diffs[name] = np.asarray(corr_toas[f'toa_b{board_a}'] - corr_toas[f'toa_b{board_b}'])\n",
    "\n",
    "        hists = {}\n",
    "        for key in diffs.keys():\n",
    "            hists[key] = hist.Hist(hist.axis.Regular(80, -1.2, 1.2, name=\"TWC_delta_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "            hists[key].fill(diffs[key])\n",
    "\n",
    "        try:\n",
    "            fit_params_lmfit = {}\n",
    "            for key in hists.keys():\n",
    "                params = helper.lmfit_gaussfit_with_pulls(diffs[key], hists[key], std_range_cut=0.4, width_factor=1.25, fig_title='',\n",
    "                                                    use_pred_uncert=True, no_show_fit=False, no_draw=True, get_chisqure=False)\n",
    "                fit_params_lmfit[key] = params\n",
    "            del params, hists, diffs, corr_toas\n",
    "\n",
    "            resolutions = helper.return_resolution_three_board(fit_params_lmfit, var=list(fit_params_lmfit.keys()), board_list=board_to_analyze)\n",
    "\n",
    "            if any(np.isnan(val) for key, val in resolutions.items()):\n",
    "                print('fit results is not good, skipping this iteration')\n",
    "                continue\n",
    "\n",
    "            for key in resolutions.keys():\n",
    "                sum_arr[key] += resolutions[key]\n",
    "                sum_square_arr[key] += resolutions[key]**2\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        except:\n",
    "            print('Failed, skipping')\n",
    "            del hists, diffs, corr_toas\n",
    "\n",
    "    if counter != 0:\n",
    "        for idx in board_to_analyze:\n",
    "            final_dict[f'row{idx}'].append(itable['row'][idx].unique()[0])\n",
    "            final_dict[f'col{idx}'].append(itable['col'][idx].unique()[0])\n",
    "\n",
    "        for key in sum_arr.keys():\n",
    "            mean = sum_arr[key]/counter\n",
    "            std = np.sqrt((1/(counter-1))*(sum_square_arr[key]-counter*(mean**2)))\n",
    "            final_dict[f'res{key}'].append(mean)\n",
    "            final_dict[f'err{key}'].append(std)\n",
    "    else:\n",
    "        print('Track is not validate for bootstrapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(data=final_dict)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel ID selection based on the event counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_dict = {\n",
    "    # board ID: [row, col]\n",
    "    0: [ 9, 5],\n",
    "    1: [ 9, 5],\n",
    "    2: [ 9, 5],\n",
    "    3: [ -1, -1],\n",
    "}\n",
    "\n",
    "filtered_group = helper.pixel_filter(df, pix_dict)\n",
    "filtered_group = helper.singlehit_event_clear(filtered_group)\n",
    "filtered_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pix_selected = helper.return_hist(filtered_group, chip_names, chip_labels, hist_bins=[100, 128, 256])\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_pix_selected, chip_names[0], chip_fignames[0], chip_figtitles[0],\n",
    "                                fig_path, save=False, show=True, tag=\"inclusive\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[0][0]}, {pix_dict[0][1]})\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_pix_selected, chip_names[1], chip_fignames[1], chip_figtitles[1],\n",
    "                                fig_path, save=False, show=True, tag=\"inclusive\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[1][0]}, {pix_dict[1][1]})\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_pix_selected, chip_names[2], chip_fignames[2], chip_figtitles[2],\n",
    "                                fig_path, save=False, show=True, tag=\"inclusive\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[2][0]}, {pix_dict[2][1]})\", slide_friendly=True)\n",
    "\n",
    "helper.plot_1d_TDC_histograms(h_pix_selected, chip_names[3], chip_fignames[3], chip_figtitles[3],\n",
    "                                fig_path, save=False, show=True, tag=\"inclusive\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[3][0]}, {pix_dict[3][1]})\", slide_friendly=True)\n",
    "\n",
    "del h_pix_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDC cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom filtering criteria for each board\n",
    "tdc_cuts = {\n",
    "    # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "    0: [filtered_group.loc[filtered_group['board'] == 0]['cal'].mode()[0]-3, filtered_group.loc[filtered_group['board'] == 0]['cal'].mode()[0]+3,  100, 500,  0, 600],\n",
    "    1: [filtered_group.loc[filtered_group['board'] == 1]['cal'].mode()[0]-3, filtered_group.loc[filtered_group['board'] == 1]['cal'].mode()[0]+3,   0,  1100, 0, 600], # pixel (15, 6), (15, 7) Sep 28th data\n",
    "    2: [filtered_group.loc[filtered_group['board'] == 2]['cal'].mode()[0]-3, filtered_group.loc[filtered_group['board'] == 2]['cal'].mode()[0]+3,   0,  1100, 0, 600],\n",
    "    # 3: [filtered_group.loc[filtered_group['board'] == 3]['cal'].mode()[0]-3, filtered_group.loc[filtered_group['board'] == 3]['cal'].mode()[0]+3,   0,  1100, 0, 600],\n",
    "}\n",
    "# tdc_cuts = {\n",
    "#     # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "#     0: [200, 210,   0, 1100,   0, 600],\n",
    "#     # 1: [173, 180, 250,  325,   0, 600], # pixel (15, 8) Sep 28th data\n",
    "#     # 1: [173, 180, 262,  337,   0, 600], # pixel (15, 9) Sep 28th data\n",
    "#     1: [173, 180, 100,  500,   0, 600],\n",
    "#     2: [160, 220,   0, 1100,   0, 600],\n",
    "#     3: [189, 195,   0, 1000,   0, 600], # pixel ()\n",
    "# }\n",
    "\n",
    "tdc_filtered_df = helper.tdc_event_selection(filtered_group, tdc_cuts)\n",
    "tdc_filtered_df = helper.singlehit_event_clear(tdc_filtered_df)\n",
    "tdc_filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tdc_selection = helper.return_hist(tdc_filtered_df, chip_names, chip_labels, hist_bins=[100, 128, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_1d_TDC_histograms(h_tdc_selection, chip_names[0], chip_fignames[0], chip_figtitles[0],\n",
    "                                fig_path, save=False, show=True, tag=\"after_tdc_cut\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[0][0]}, {pix_dict[0][1]}) after TDC cut\", slide_friendly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_1d_TDC_histograms(h_tdc_selection, chip_names[1], chip_fignames[1], chip_figtitles[1],\n",
    "                                fig_path, save=False, show=True, tag=\"\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[1][0]}, {pix_dict[1][1]}) after TDC cut\", slide_friendly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_1d_TDC_histograms(h_tdc_selection, chip_names[2], chip_fignames[2], chip_figtitles[2],\n",
    "                                fig_path, save=False, show=True, tag=\"\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[2][0]}, {pix_dict[2][1]}) after TDC cut\", slide_friendly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_1d_TDC_histograms(h_tdc_selection, chip_names[3], chip_fignames[3], chip_figtitles[3],\n",
    "                                fig_path, save=False, show=True, tag=\"\",\n",
    "                                title_tag=f\", Pixel ({pix_dict[3][0]}, {pix_dict[3][1]}) after TDC cut\", slide_friendly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_tdc_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert TDC code to TDC time in [ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc_filtered_df = tdc_filtered_df.pivot(index=[\"evt\"], columns=[\"board\"], values=[\"row\", \"col\", \"toa\", \"tot\", \"cal\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_to_analyze = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'evt': tdc_filtered_df['evt'].unique(),\n",
    "}\n",
    "\n",
    "for idx in board_to_analyze:\n",
    "    bins = 3.125/tdc_filtered_df['cal'][idx].mean()\n",
    "    d[f'toa_b{str(idx)}'] = 12.5 - tdc_filtered_df['toa'][idx] * bins\n",
    "    d[f'tot_b{str(idx)}'] = (2*tdc_filtered_df['tot'][idx] - np.floor(tdc_filtered_df['tot'][idx]/32)) * bins\n",
    "\n",
    "df_in_time = pd.DataFrame(data=d)\n",
    "del d, tdc_filtered_df\n",
    "\n",
    "corr_toas = helper.three_board_iterative_timewalk_correction(df_in_time, 5, 3, board_list=board_to_analyze)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Time Walk Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_toa_b0 = (0.5*(df_in_time['toa_b1'] + df_in_time['toa_b2']) - df_in_time['toa_b0']).values\n",
    "del_toa_b1 = (0.5*(df_in_time['toa_b0'] + df_in_time['toa_b2']) - df_in_time['toa_b1']).values\n",
    "del_toa_b3 = (0.5*(df_in_time['toa_b0'] + df_in_time['toa_b1']) - df_in_time['toa_b2']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_b0 = np.polyfit(df_in_time['tot_b0'].values, del_toa_b0, 3)\n",
    "poly_func_b0 = np.poly1d(coeff_b0)\n",
    "\n",
    "coeff_b1 = np.polyfit(df_in_time['tot_b1'].values, del_toa_b1, 3)\n",
    "poly_func_b1 = np.poly1d(coeff_b1)\n",
    "\n",
    "coeff_b3 = np.polyfit(df_in_time['tot_b2'].values, del_toa_b3, 3)\n",
    "poly_func_b3 = np.poly1d(coeff_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[0].scatter(df_in_time['tot_b0'].values,  del_toa_b0, label='data')\n",
    "axes[0].plot(df_in_time['tot_b0'].values, poly_func_b0(df_in_time['tot_b0'].values), 'r.', label='fit')\n",
    "axes[0].set_xlabel('TOT time [ns]')\n",
    "axes[0].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]' )\n",
    "axes[0].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)\n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[1].scatter(df_in_time['tot_b1'].values,  del_toa_b1, label='data')\n",
    "axes[1].plot(df_in_time['tot_b1'].values, poly_func_b1(df_in_time['tot_b1'].values), 'r.', label='fit')\n",
    "axes[1].set_xlabel('TOT time [ns]')\n",
    "axes[1].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[1].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[2].scatter(df_in_time['tot_b2'].values,  del_toa_b3, label='data')\n",
    "axes[2].plot(df_in_time['tot_b2'].values, poly_func_b3(df_in_time['tot_b2'].values), 'r.', label='fit')\n",
    "axes[2].set_xlabel('TOT time [ns]')\n",
    "axes[2].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_toas = helper.three_board_iterative_timewalk_correction(df_in_time, 5, 3, board_list=[0, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_toas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_del_toa_b0 = (0.5*(corr_toas['toa_b1'] + corr_toas['toa_b2']) - corr_toas['toa_b0'])\n",
    "nth_del_toa_b1 = (0.5*(corr_toas['toa_b0'] + corr_toas['toa_b2']) - corr_toas['toa_b1'])\n",
    "nth_del_toa_b3 = (0.5*(corr_toas['toa_b0'] + corr_toas['toa_b1']) - corr_toas['toa_b2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_b0 = np.polyfit(df_in_time['tot_b0'].values, nth_del_toa_b0, 3)\n",
    "poly_func_b0 = np.poly1d(coeff_b0)\n",
    "\n",
    "coeff_b1 = np.polyfit(df_in_time['tot_b1'].values, nth_del_toa_b1, 3)\n",
    "poly_func_b1 = np.poly1d(coeff_b1)\n",
    "\n",
    "coeff_b3 = np.polyfit(df_in_time['tot_b2'].values, nth_del_toa_b3, 3)\n",
    "poly_func_b3 = np.poly1d(coeff_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[0].scatter(df_in_time['tot_b0'].values,  nth_del_toa_b0, label='data')\n",
    "axes[0].plot(df_in_time['tot_b0'].values, poly_func_b0(df_in_time['tot_b0'].values), 'r.', label='fit')\n",
    "axes[0].set_xlabel('TOT time [ns]')\n",
    "axes[0].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]' )\n",
    "axes[0].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)\n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[1].scatter(df_in_time['tot_b1'].values,  nth_del_toa_b1, label='data')\n",
    "axes[1].plot(df_in_time['tot_b1'].values, poly_func_b1(df_in_time['tot_b1'].values), 'r.', label='fit')\n",
    "axes[1].set_xlabel('TOT time [ns]')\n",
    "axes[1].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[1].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[2].scatter(df_in_time['tot_b2'].values,  nth_del_toa_b3, label='data')\n",
    "axes[2].plot(df_in_time['tot_b2'].values, poly_func_b3(df_in_time['tot_b2'].values), 'r.', label='fit')\n",
    "axes[2].set_xlabel('TOT time [ns]')\n",
    "axes[2].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrTOA_b0 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "corrTOA_b1 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "corrTOA_b3 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "corrTOA_b0.fill(corr_toas[0])\n",
    "corrTOA_b1.fill(corr_toas[1])\n",
    "corrTOA_b3.fill(corr_toas[2])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b0, ax=axes[0], lw=2)\n",
    "axes[0].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[0].set_ylabel('Entries')\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)\n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b1, ax=axes[1], lw=2)\n",
    "axes[1].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[1].set_ylabel('Entries')\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b3, ax=axes[2], lw=2)\n",
    "axes[2].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[2].set_ylabel('Entries')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict = {\n",
    "    'evt': df_in_time['evt'].values,\n",
    "    'corr_toa_b0': corr_toas[0],\n",
    "    'corr_toa_b1': corr_toas[1],\n",
    "    'corr_toa_b3': corr_toas[2],\n",
    "}\n",
    "\n",
    "df_in_time_corr = pd.DataFrame(tmp_dict)\n",
    "del tmp_dict\n",
    "df_in_time_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_b01 = df_in_time_corr['corr_toa_b0'] - df_in_time_corr['corr_toa_b1']\n",
    "diff_b03 = df_in_time_corr['corr_toa_b0'] - df_in_time_corr['corr_toa_b3']\n",
    "diff_b13 = df_in_time_corr['corr_toa_b1'] - df_in_time_corr['corr_toa_b3']\n",
    "\n",
    "dTOA_b01 = hist.Hist(hist.axis.Regular(80, diff_b01.mean().round(2)-0.8, diff_b01.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_b03 = hist.Hist(hist.axis.Regular(80, diff_b03.mean().round(2)-0.8, diff_b03.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_b13 = hist.Hist(hist.axis.Regular(80, diff_b13.mean().round(2)-0.8, diff_b13.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_b01.fill(diff_b01)\n",
    "dTOA_b03.fill(diff_b03)\n",
    "dTOA_b13.fill(diff_b13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit using lmfit GaussianModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b01, dTOA_b01, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 1',\n",
    "                                          use_pred_uncert=True, no_show_fit=False, no_draw=False, get_chisqure=False)\n",
    "fit_params_lmfit.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b03, dTOA_b03, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 3',\n",
    "                                          use_pred_uncert=True, no_show_fit=False, no_draw=False, get_chisqure=False)\n",
    "fit_params_lmfit.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b13, dTOA_b13, std_range_cut=0.4, width_factor=1.25, fig_title='Board 1 - Board 3',\n",
    "                                          use_pred_uncert=True, no_show_fit=False, no_draw=False, get_chisqure=False)\n",
    "fit_params_lmfit.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0,err_b0 = helper.return_resolution_three_board(fit_params_lmfit[0][0], fit_params_lmfit[0][1],\n",
    "                                            fit_params_lmfit[1][0], fit_params_lmfit[1][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1])\n",
    "res_b1,err_b1 = helper.return_resolution_three_board(fit_params_lmfit[0][0], fit_params_lmfit[0][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1],\n",
    "                                            fit_params_lmfit[1][0], fit_params_lmfit[1][1])\n",
    "res_b3,err_b3 = helper.return_resolution_three_board(fit_params_lmfit[1][0], fit_params_lmfit[1][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1],\n",
    "                                            fit_params_lmfit[0][0], fit_params_lmfit[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0:.2f} ps, error: {err_b0:.2f} ps')\n",
    "print(f'Board 1: {res_b1:.2f} ps, error: {err_b1:.2f} ps')\n",
    "print(f'Board 3: {res_b3:.2f} ps, error: {err_b3:.2f} ps')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-polynomial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2D_2_fit(M, *args):\n",
    "    x, y = M\n",
    "    return helper.poly2D(2, x, y, *args)\n",
    "\n",
    "def poly2D_3_fit(M, *args):\n",
    "    x, y = M\n",
    "    return helper.poly2D(3, x, y, *args)\n",
    "\n",
    "def poly2D_4_fit(M, *args):\n",
    "    x, y = M\n",
    "    return helper.poly2D(4, x, y, *args)\n",
    "\n",
    "def poly3D_2_fit(M, *args):\n",
    "    x, y, z = M\n",
    "    return helper.poly3D(2, x, y, z, *args)\n",
    "\n",
    "helper.poly2D(0, df_in_time['tot_b0'], df_in_time['tot_b1'], 2)                              #  pol: z = 2\n",
    "helper.poly2D(1, df_in_time['tot_b0'], df_in_time['tot_b1'], 2, 4, 1)                        #  pol: z = 2 + 4x + y\n",
    "helper.poly2D(2, df_in_time['tot_b0'], df_in_time['tot_b1'], 2, 0.2, -0.9, 0.3, -0.1, 0.01)  #  pol: z = 2 + 0.2x - 0.9x^2 + 0.3y - 0.1xy + 0.01y^2\n",
    "\n",
    "import scipy.optimize as opt\n",
    "x = df_in_time['tot_b0']\n",
    "y = df_in_time['tot_b1']\n",
    "z = df_in_time['tot_b3']\n",
    "data = df_in_time['toa_b0'] - df_in_time['toa_b1']\n",
    "data = df_in_time['toa_b0'] - df_in_time['toa_b3']\n",
    "data = df_in_time['toa_b1'] - df_in_time['toa_b3']\n",
    "initial_guess = (data.mean(), 0, 0, 0, 0, 0)\n",
    "popt, pcov = opt.curve_fit(poly2D_2_fit, (x,y), data, p0 = initial_guess)\n",
    "corr_data = helper.poly2D(2, df_in_time['tot_b0'], df_in_time['tot_b1'], *popt)\n",
    "\n",
    "initial_guess = (data.mean(), 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "popt, pcov = opt.curve_fit(poly2D_3_fit, (x,y), data, p0 = initial_guess)\n",
    "corr_data = helper.poly2D(3, df_in_time['tot_b0'], df_in_time['tot_b1'], *popt)\n",
    "\n",
    "initial_guess = (data.mean(), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "popt, pcov = opt.curve_fit(poly2D_4_fit, (x,y), data, p0 = initial_guess)\n",
    "corr_data = helper.poly2D(4, df_in_time['tot_b0'], df_in_time['tot_b1'], *popt)\n",
    "\n",
    "#initial_guess = (data.mean(), 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "#popt, pcov = opt.curve_fit(poly3D_2_fit, (x,y,z), data, p0 = initial_guess)\n",
    "#corr_data = poly3D(2, df_in_time['tot_b0'], df_in_time['tot_b1'], df_in_time['tot_b3'], *popt)\n",
    "\n",
    "dTOA_test = hist.Hist(hist.axis.Regular(80, corr_data.mean().round(2)-0.8, corr_data.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_test.fill(corr_data)\n",
    "dTOA_test\n",
    "\n",
    "params = helper.lmfit_gaussfit_with_pulls(corr_data, dTOA_test, std_range_cut=0.4, width_factor=1, fig_title='Board 1 - Board 3', use_pred_uncert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Time Walk Correction -  Pairwise Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dense_model(numpars=2, print_summary=False):\n",
    "    input  = Input(shape=(numpars,), name='input')\n",
    "    dense1 = Dense(8, activation='relu', name='dense1',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(input)\n",
    "    dense2 = Dense(8, activation='relu', name='dense2',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense1)\n",
    "    output = Dense(1, activation='linear', name='output',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense2)\n",
    "    model  = Model(inputs=[input], outputs=output, name=\"simple_dense_NN\")\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    if(print_summary): print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = return_dense_model(print_summary=True)\n",
    "del model\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "\n",
    "def return_combined_dense_model(numpars=3, print_summary=False):\n",
    "    input  = Input(shape=(numpars,), name='input')\n",
    "    dense1 = Dense(8, activation='relu', name='dense1',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(input)\n",
    "    dense2 = Dense(8, activation='relu', name='dense2',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense1)\n",
    "    output = Dense(3, activation='linear', name='output',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense2)\n",
    "    model  = Model(inputs=[input], outputs=output, name=\"simple_dense_NN\")\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    if(print_summary): print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = return_combined_dense_model(print_summary=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etroc_regression_using_NN(\n",
    "        input_df: pd.DataFrame,\n",
    "        variables: list,\n",
    "        data_tag: str,\n",
    "        extra_tag: str,\n",
    "        board_tag: str,\n",
    "        ensemble_count: int,\n",
    "        figure_title: str,\n",
    "        do_plotting: bool,\n",
    "        epochs: int = 300,\n",
    "    ):\n",
    "    filename = f'{data_tag}_weights_{extra_tag}_{board_tag}'\n",
    "\n",
    "    for en_idx in range(ensemble_count):\n",
    "        model = return_dense_model(numpars=2)\n",
    "        checkpointer = ModelCheckpoint(f'models/NNRun{en_idx}_{filename}.hdf5', verbose=0, save_best_only=True, monitor=\"val_loss\")\n",
    "        term = tf.keras.callbacks.TerminateOnNaN()\n",
    "        escb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=0)\n",
    "\n",
    "        shuffled_df = input_df.sample(frac=1)\n",
    "\n",
    "        history = model.fit(\n",
    "            shuffled_df[[variables[0], variables[1]]].values,\n",
    "            (shuffled_df[variables[2]]-shuffled_df[variables[3]]).values,\n",
    "            validation_split=0.3,\n",
    "            epochs=epochs,\n",
    "            callbacks=[checkpointer,term,escb],\n",
    "            verbose=0)\n",
    "\n",
    "        del model\n",
    "\n",
    "        if (do_plotting):\n",
    "            #plot the loss and validation loss of the dataset\n",
    "            fig, ax = plt.subplots(figsize=(15, 10), dpi=50)\n",
    "            hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "            ax.set_title(f'Model {en_idx}, {figure_title}', loc=\"right\", size=25)\n",
    "            plt.plot(history.history['loss'], label='Train data')\n",
    "            plt.plot(history.history['val_loss'], label='Validation data')\n",
    "            plt.ylabel('Loss (MSE)')\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"models/NNRun{en_idx}_{filename}.png\")\n",
    "            plt.show()\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "\n",
    "def etroc_regression_using_combined_NN(\n",
    "        input_df: pd.DataFrame,\n",
    "        variables: list,\n",
    "        data_tag: str,\n",
    "        extra_tag: str,\n",
    "        ensemble_count: int,\n",
    "        figure_title: str,\n",
    "        do_plotting: bool,\n",
    "        epochs: int = 300,\n",
    "    ):\n",
    "    filename = f'{data_tag}_weights_{extra_tag}'\n",
    "\n",
    "    inner_df = input_df.copy()\n",
    "    inner_df['delta01'] = inner_df[variables[3]] - inner_df[variables[4]]\n",
    "    inner_df['delta03'] = inner_df[variables[3]] - inner_df[variables[5]]\n",
    "    inner_df['delta13'] = inner_df[variables[4]] - inner_df[variables[5]]\n",
    "\n",
    "    for en_idx in range(ensemble_count):\n",
    "        model = return_combined_dense_model(numpars=3)\n",
    "        checkpointer = ModelCheckpoint(f'models/CombinedNNRun{en_idx}_{filename}.hdf5', verbose=0, save_best_only=True, monitor=\"val_loss\")\n",
    "        term = tf.keras.callbacks.TerminateOnNaN()\n",
    "        escb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=0)\n",
    "\n",
    "        shuffled_df = inner_df.sample(frac=1)\n",
    "\n",
    "        history = model.fit(\n",
    "            shuffled_df[[variables[0], variables[1], variables[2]]].values,\n",
    "            shuffled_df[['delta01', 'delta03', 'delta13']].values,\n",
    "            validation_split=0.3,\n",
    "            epochs=epochs,\n",
    "            callbacks=[checkpointer,term,escb],\n",
    "            verbose=0)\n",
    "\n",
    "        del model\n",
    "\n",
    "        if (do_plotting):\n",
    "            #plot the loss and validation loss of the dataset\n",
    "            fig, ax = plt.subplots(figsize=(15, 10), dpi=50)\n",
    "            hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "            ax.set_title(f'Model {en_idx}, {figure_title}', loc=\"right\", size=25)\n",
    "            plt.plot(history.history['loss'], label='Train data')\n",
    "            plt.plot(history.history['val_loss'], label='Validation data')\n",
    "            plt.ylabel('Loss (MSE)')\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"models/CombinedNNRun{en_idx}_{filename}.png\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_count = 10\n",
    "data_tag = 'tb_sep28_offset24_Bottom_offset6'\n",
    "\n",
    "# extra_tag = \"WB156_CALNarrow_TOA275to350forWB_NoTOTcut\"\n",
    "extra_tag = \"WB157_CALNarrow_TOA275to350forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB158_CALNarrow_TOA250to325forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB159_CALNarrow_TOA262to337forWB_NoTOTcut\"\n",
    "\n",
    "# extra_tag = \"WB156_CALNarrow_TOA100to500forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB157_CALNarrow_TOA100to500forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB158_CALNarrow_TOA100to500forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB159_CALNarrow_TOA100to500forWB_NoTOTcut\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NN Training (skip if you don't want to do training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['tot_b0', 'tot_b1', 'toa_b0', 'toa_b1']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b01\", ensemble_count, figure_title='Board 0 - Board 1', do_plotting=True)\n",
    "\n",
    "vars = ['tot_b0', 'tot_b3', 'toa_b0', 'toa_b3']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b03\", ensemble_count, figure_title='Board 0 - Board 3', do_plotting=True)\n",
    "\n",
    "vars = ['tot_b1', 'tot_b3', 'toa_b1', 'toa_b3']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b13\", ensemble_count, figure_title='Board 1 - Board 3', do_plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['tot_b0', 'tot_b1', 'tot_b3', 'toa_b0', 'toa_b1', 'toa_b3']\n",
    "etroc_regression_using_combined_NN(df_in_time, vars, data_tag, extra_tag, ensemble_count, figure_title='Combined Loss', do_plotting=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load individual trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b01 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b01'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b01.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b01.predict(df_in_time[['tot_b0', 'tot_b1']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b01.predict(df_in_time[['tot_b0', 'tot_b1']].values, verbose=0).flatten()\n",
    "del model_b01\n",
    "Y_pred_b01 = Y_pred/ensemble_count\n",
    "\n",
    "model_b03 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b03'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b03.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b03.predict(df_in_time[['tot_b0', 'tot_b3']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b03.predict(df_in_time[['tot_b0', 'tot_b3']].values, verbose=0).flatten()\n",
    "del model_b03\n",
    "Y_pred_b03 = Y_pred/ensemble_count\n",
    "\n",
    "model_b13 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b13'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b13.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b13.predict(df_in_time[['tot_b1', 'tot_b3']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b13.predict(df_in_time[['tot_b1',  'tot_b3']].values, verbose=0).flatten()\n",
    "del model_b13\n",
    "Y_pred_b13 = Y_pred/ensemble_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b01 = (df_in_time['toa_b0']-df_in_time['toa_b1']).values-Y_pred_b01\n",
    "data_b03 = (df_in_time['toa_b0']-df_in_time['toa_b3']).values-Y_pred_b03\n",
    "data_b13 = (df_in_time['toa_b1']-df_in_time['toa_b3']).values-Y_pred_b13\n",
    "\n",
    "dTOA_NN_b01 = hist.Hist(hist.axis.Regular(50, data_b01.mean().round(2)-0.5, data_b01.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b03 = hist.Hist(hist.axis.Regular(50, data_b03.mean().round(2)-0.5, data_b03.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b13 = hist.Hist(hist.axis.Regular(50, data_b13.mean().round(2)-0.5, data_b13.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_NN_b01.fill(data_b01)\n",
    "dTOA_NN_b03.fill(data_b03)\n",
    "dTOA_NN_b13.fill(data_b13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load combined trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined = return_combined_dense_model(numpars=3)\n",
    "filename = f'{data_tag}_weights_{extra_tag}'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_combined.load_weights(f'models/CombinedNNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_combined.predict(df_in_time[['tot_b0', 'tot_b1', 'tot_b3']].values, verbose=0)\n",
    "    else: Y_pred += model_combined.predict(df_in_time[['tot_b0', 'tot_b1', 'tot_b3']].values, verbose=0)\n",
    "del model_combined\n",
    "Y_pred_combined = Y_pred/ensemble_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b01_combined = (df_in_time['toa_b0']-df_in_time['toa_b1']).values-Y_pred_combined[:,0]\n",
    "data_b03_combined = (df_in_time['toa_b0']-df_in_time['toa_b3']).values-Y_pred_combined[:,1]\n",
    "data_b13_combined = (df_in_time['toa_b1']-df_in_time['toa_b3']).values-Y_pred_combined[:,2]\n",
    "\n",
    "dTOA_NN_b01_combined = hist.Hist(hist.axis.Regular(50, data_b01_combined.mean().round(2)-0.5, data_b01_combined.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b03_combined = hist.Hist(hist.axis.Regular(50, data_b03_combined.mean().round(2)-0.5, data_b03_combined.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b13_combined = hist.Hist(hist.axis.Regular(50, data_b13_combined.mean().round(2)-0.5, data_b13_combined.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_NN_b01_combined.fill(data_b01_combined)\n",
    "dTOA_NN_b03_combined.fill(data_b03_combined)\n",
    "dTOA_NN_b13_combined.fill(data_b13_combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit using lmfit GaussianModel with NN outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit_NN = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b01, dTOA_NN_b01, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b03, dTOA_NN_b03, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b13, dTOA_NN_b13, std_range_cut=0.4, width_factor=1.25, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit_NN_combined = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b01_combined, dTOA_NN_b01_combined, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN_combined.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b03_combined, dTOA_NN_b03_combined, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN_combined.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b13_combined, dTOA_NN_b13_combined, std_range_cut=0.4, width_factor=1, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN_combined.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0_NN,err_b0_NN = helper.return_resolution_ps(fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1],\n",
    "                                            fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1])\n",
    "res_b1_NN,err_b1_NN = helper.return_resolution_ps(fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1],\n",
    "                                            fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1])\n",
    "res_b3_NN,err_b3_NN = helper.return_resolution_ps(fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1],\n",
    "                                            fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0_NN:.2f} ps, error: {err_b0_NN:.2f} ps')\n",
    "print(f'Board 1: {res_b1_NN:.2f} ps, error: {err_b1_NN:.2f} ps')\n",
    "print(f'Board 3: {res_b3_NN:.2f} ps, error: {err_b3_NN:.2f} ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0_NN_combined,err_b0_NN_combined = helper.return_resolution_ps(fit_params_lmfit_NN_combined[0][0], fit_params_lmfit_NN_combined[0][1],\n",
    "                                            fit_params_lmfit_NN_combined[1][0], fit_params_lmfit_NN_combined[1][1],\n",
    "                                            fit_params_lmfit_NN_combined[2][0], fit_params_lmfit_NN_combined[2][1])\n",
    "res_b1_NN_combined,err_b1_NN_combined = helper.return_resolution_ps(fit_params_lmfit_NN_combined[0][0], fit_params_lmfit_NN_combined[0][1],\n",
    "                                            fit_params_lmfit_NN_combined[2][0], fit_params_lmfit_NN_combined[2][1],\n",
    "                                            fit_params_lmfit_NN_combined[1][0], fit_params_lmfit_NN_combined[1][1])\n",
    "res_b3_NN_combined,err_b3_NN_combined = helper.return_resolution_ps(fit_params_lmfit_NN_combined[1][0], fit_params_lmfit_NN_combined[1][1],\n",
    "                                            fit_params_lmfit_NN_combined[2][0], fit_params_lmfit_NN_combined[2][1],\n",
    "                                            fit_params_lmfit_NN_combined[0][0], fit_params_lmfit_NN_combined[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0_NN_combined:.2f} ps, error: {err_b0_NN_combined:.2f} ps')\n",
    "print(f'Board 1: {res_b1_NN_combined:.2f} ps, error: {err_b1_NN_combined:.2f} ps')\n",
    "print(f'Board 3: {res_b3_NN_combined:.2f} ps, error: {err_b3_NN_combined:.2f} ps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Top', 'Middle', 'Bottom']\n",
    "\n",
    "board_resols = {\n",
    "    'Single board TWC': ((res_b1, res_b0, res_b3), (err_b1, err_b0, err_b3)),\n",
    "    'Pairwise TWC': ((res_b1_NN, res_b0_NN, res_b3_NN), (err_b1_NN, err_b0_NN, err_b3_NN)),\n",
    "    # 'Combined TWC': ((res_b1_NN_combined, res_b0_NN_combined, res_b3_NN_combined), (err_b1_NN_combined, err_b0_NN_combined, err_b3_NN_combined)),\n",
    "}\n",
    "\n",
    "x = np.arange(len(names))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6), layout='constrained')\n",
    "\n",
    "for attribute, measurement in board_resols.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.barh(x + offset, measurement[0], width, edgecolor='white', xerr=measurement[1], label=attribute)\n",
    "    multiplier += 1\n",
    "\n",
    "rectangle = mpatch.Rectangle((55, 0.2), 40, 0.6, edgecolor='black', facecolor=\"none\", linewidth=1.5)\n",
    "ax.add_patch(rectangle)\n",
    "rx, ry = rectangle.get_xy()\n",
    "cx = rx + rectangle.get_width()/30.\n",
    "cy = ry + rectangle.get_height()/2.\n",
    "\n",
    "text_for_top = 'Top: FFF corner, Wire bonded 2x2, W36-IP7-13 (HPK, split3), offset 24, HV=210V'\n",
    "text_for_mid = 'Middle (Trigger): FFF corner, Bump bonded 16x16, W13 4-5 (FBK), offset 24, HV=210V'\n",
    "text_for_bot = 'Bottom: FFF corner, Bump bonded 16x16, W16 P6 (HPK), offset 6, HV=210V'\n",
    "\n",
    "ax.annotate(f\"{text_for_top} \\n {text_for_mid} \\n {text_for_bot}\", (cx, cy),\n",
    "            color='black', weight='bold', fontsize=11, ha='left', va='center')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "ax.set_title(rf'TOA$_{{{names[0]}}} \\in \\left[{tdc_cuts[1][2]}, {tdc_cuts[1][3]}\\right]$', loc=\"right\", size=20)\n",
    "ax.set_xlabel('Time Resolution [ps]', fontsize=20)\n",
    "ax.set_yticks(x + width, names)\n",
    "ax.legend(loc='lower right', fontsize=18)\n",
    "ax.set_xlim(30, 100)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
