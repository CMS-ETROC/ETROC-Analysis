{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# zlib License\n",
    "#\n",
    "# (C) 2023 Murtaza Safdari <musafdar@cern.ch>, Jongho Lee <jongho.lee@cern.ch>\n",
    "#\n",
    "# This software is provided 'as-is', without any express or implied\n",
    "# warranty.  In no event will the authors be held liable for any damages\n",
    "# arising from the use of this software.\n",
    "#\n",
    "# Permission is granted to anyone to use this software for any purpose,\n",
    "# including commercial applications, and to alter it and redistribute it\n",
    "# freely, subject to the following restrictions:\n",
    "#\n",
    "# 1. The origin of this software must not be misrepresented; you must not\n",
    "#    claim that you wrote the original software. If you use this software\n",
    "#    in a product, an acknowledgment in the product documentation would be\n",
    "#    appreciated but is not required.\n",
    "# 2. Altered source versions must be plainly marked as such, and must not be\n",
    "#    misrepresented as being the original software.\n",
    "# 3. This notice may not be removed or altered from any source distribution.\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beamtest_analysis_helper import etroc2_analysis_helper\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import hist\n",
    "import mplhep as hep\n",
    "hep.style.use('CMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!\n",
    "# It is very important to correctly set the chip name, this value is stored with the data\n",
    "\n",
    "chip_names = [\"ET2_W36_IP7_13_HV210V_offset24\",\"ET2_EPIR_1_1_HV210V_offset24\", \"ET2_CNM_1_3_HV210V_offset6\"]\n",
    "chip_fignames = chip_names\n",
    "chip_figtitles = [\"ETROC2 WB W36 IP7-13 HV210V OS:24\",\"(Trigger) ETROC2 BB EPIR 1-1 HV210V OS:24\", \"ETROC2 BB CNM 1-3 HV210V OS:6\"]\n",
    "\n",
    "chip_labels= [\"1\",\"0\",\"3\"]\n",
    "\n",
    "today = datetime.date.today().isoformat()\n",
    "fig_outdir = Path('../../ETROC-figures')\n",
    "fig_outdir = fig_outdir / (today + '_Array_Test_Results')\n",
    "fig_outdir.mkdir(exist_ok=True)\n",
    "fig_path = str(fig_outdir)\n",
    "\n",
    "# path_pattern = f\"*2023-09-21_Array_Test_Results/SelfTrigger_bottom_Readout_topbottom_1\"\n",
    "# path_pattern = f\"./testbeam_sep24/SelfTrigger_ET2_CNM_BATCH_1_3_Readout_ET2_EPIR_BATCH1_1_ET2_W36_IP7_13_ET2_CNM_BATCH1_3_loop_*.pqt\"\n",
    "path_pattern = \"./highpower_offset6/SelfTrigger_ET2_EPIR_BATCH1_1_Readout_ET2_W36_IP7_13_ET2_CNM_BATCH1_3_offset6_highpower_FINALRUN_loop_*.pqt\"\n",
    "\n",
    "helper = etroc2_analysis_helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "\n",
    "fig = plt.figure(dpi=50, figsize=(5,5))\n",
    "gs = fig.add_gridspec(1,1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "ax0.plot([1, 0], [1, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(path_pattern)\n",
    "files = natsorted(files)\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for ifile in files:\n",
    "    tmp_df = pd.read_parquet(ifile)\n",
    "    if tmp_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Group the DataFrame by 'evt' and count unique 'board' values in each group\n",
    "    unique_board_counts = tmp_df.groupby('evt')['board'].nunique()\n",
    "\n",
    "    ## event has three unique board ID\n",
    "    event_numbers_with_three_unique_boards = unique_board_counts[unique_board_counts == 3].index\n",
    "    subset_df = tmp_df[tmp_df['evt'].isin(event_numbers_with_three_unique_boards)]\n",
    "    subset_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    del tmp_df\n",
    "    if subset_df.empty: \n",
    "        continue\n",
    "    \n",
    "    dataframes.append(subset_df)\n",
    "\n",
    "df = pd.concat(dataframes)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "del dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_inclusive = helper.return_hist(df, chip_names, chip_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_inclusive, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_inclusive, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_inclusive, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.making_heatmap_byPandas(df, chip_labels, chip_figtitles, \"inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.making_3d_heatmap_byPandas(df, chip_labels, chip_figtitles, \"inclusive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event counter and 2D heatmap based on WB pixel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_info = [1, 15, 6]\n",
    "simple_filtered_df = helper.find_maximum_event_combination(df, pix_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.making_heatmap_byPandas(simple_filtered_df, chip_labels, chip_figtitles, figtitle_tag=\"WB pixel (15, 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simple_filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel ID selection based on the event counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_dict = {\n",
    "    # board ID: [row, col]\n",
    "    0: [ 2, 6],\n",
    "    1: [15, 6],\n",
    "    2: [ 0, 0],\n",
    "    3: [ 3, 5],\n",
    "}\n",
    "\n",
    "filtered_group = helper.pixel_filter(df, pix_dict)\n",
    "filtered_group = helper.singlehit_event_clear_func(filtered_group)\n",
    "filtered_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pix_selected = helper.return_hist(filtered_group, chip_names, chip_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_pix_selected, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", Pixel (15, 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_pix_selected, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", Pixel (2, 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_pix_selected, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", Pixel (3, 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_pix_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDC cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom filtering criteria for each board\n",
    "tdc_cuts = {    \n",
    "    # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "    0: [199, 205,   0, 1100,   0, 600],\n",
    "    1: [200, 210, 275,  350,   0, 600],\n",
    "    2: [160, 220,   0, 1100,   0, 600],\n",
    "    3: [189, 195,   0, 1000,   0, 600],\n",
    "}\n",
    "\n",
    "tdc_filtered_df = helper.tdc_event_selection(filtered_group, tdc_cuts)\n",
    "tdc_filtered_df = helper.singlehit_event_clear_func(tdc_filtered_df)\n",
    "tdc_filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_filter(group):\n",
    "    return group.sort_values(by=['board'], ascending=True)\n",
    "\n",
    "def distance_filter(group, distance):\n",
    "    board0_row = group[(group[\"board\"] == 0)]\n",
    "    board3_row = group[(group[\"board\"] == 3)]\n",
    "    board0_col = group[(group[\"board\"] == 0)]\n",
    "    board3_col = group[(group[\"board\"] == 3)]\n",
    "\n",
    "    if not board0_row.empty and not board3_row.empty and not board0_col.empty and not board3_col.empty:\n",
    "        row_index_diff = abs(board0_row[\"row\"].values[0] - board3_row[\"row\"].values[0])\n",
    "        col_index_diff = abs(board0_col[\"col\"].values[0] - board3_col[\"col\"].values[0])\n",
    "        return row_index_diff < distance and col_index_diff < distance\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# tmp_group = selected_subset_df.groupby('evt')\n",
    "# filtered_simple_group = tmp_group.filter(simple_filter, board=1, row=15, col=6)\n",
    "# filtered_simple_group.reset_index(inplace=True, drop=True)\n",
    "# del tmp_group\n",
    "\n",
    "# grouped = filtered_simple_group.groupby('evt')\n",
    "# sorted_filtered_simple_group = grouped.apply(sort_filter)\n",
    "# sorted_filtered_simple_group.reset_index(inplace=True, drop=True)\n",
    "# sorted_filtered_simple_group\n",
    "# del grouped\n",
    "\n",
    "# grouped = sorted_filtered_simple_group.groupby('evt')\n",
    "# dis_simple_group = grouped.filter(distance_filter, distance=2)\n",
    "# dis_simple_group\n",
    "\n",
    "# test_group = dis_simple_group.groupby(['board', 'row', 'col'])\n",
    "# test = test_group.size().reset_index(name='count')\n",
    "# test.to_csv('test.csv', index=False)\n",
    "\n",
    "# del filtered_simple_group,sorted_filtered_simple_group,grouped,dis_simple_group, test_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tdc_selection = helper.return_hist(tdc_filtered_df, chip_names, chip_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_tdc_selection, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True, tag=\"after_tdc_cut\", title_tag=\", Pixel (15, 6) after TDC cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_tdc_selection, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True, tag=\"\", title_tag=\", Pixel (2, 6) after TDC cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_tdc_selection, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True, tag=\"\", title_tag=\", Pixel (2, 5) after TDC cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_tdc_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert TDC code to TDC time in [ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = tdc_filtered_df\n",
    "\n",
    "pix_rows = []\n",
    "pix_cols = []\n",
    "fit_params = []\n",
    "cal_means = {boardID:{} for boardID in chip_labels}\n",
    "\n",
    "for boardID in chip_labels:\n",
    "    groups = selected_df[selected_df['board'] == int(boardID)].groupby(['row', 'col'])\n",
    "    for (row, col), group in groups:\n",
    "        \n",
    "        cal_mean = group['cal'].mean()\n",
    "        cal_means[boardID][(row, col)] = cal_mean\n",
    "\n",
    "cal_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin0 = (3.125/cal_means[\"0\"][(2, 6)])\n",
    "bin1 = (3.125/cal_means[\"1\"][(15, 6)])\n",
    "bin3 = (3.125/cal_means[\"3\"][(3, 5)])\n",
    "\n",
    "toa_in_time_b0 = 12.5 - selected_df[selected_df['board'] == 0]['toa'] * bin0\n",
    "toa_in_time_b1 = 12.5 - selected_df[selected_df['board'] == 1]['toa'] * bin1\n",
    "toa_in_time_b3 = 12.5 - selected_df[selected_df['board'] == 3]['toa'] * bin3\n",
    "\n",
    "tot_in_time_b0 = (2*selected_df[selected_df['board'] == 0]['tot'] - np.floor(selected_df[selected_df['board'] == 0]['tot']/32)) * bin0\n",
    "tot_in_time_b1 = (2*selected_df[selected_df['board'] == 1]['tot'] - np.floor(selected_df[selected_df['board'] == 1]['tot']/32)) * bin1\n",
    "tot_in_time_b3 = (2*selected_df[selected_df['board'] == 3]['tot'] - np.floor(selected_df[selected_df['board'] == 3]['tot']/32)) * bin3\n",
    "\n",
    "d = {\n",
    "    'evt': selected_df['evt'].unique(),\n",
    "    'toa_b0': toa_in_time_b0.to_numpy(),\n",
    "    'tot_b0': tot_in_time_b0.to_numpy(),\n",
    "    'toa_b1': toa_in_time_b1.to_numpy(),\n",
    "    'tot_b1': tot_in_time_b1.to_numpy(),\n",
    "    'toa_b3': toa_in_time_b3.to_numpy(),\n",
    "    'tot_b3': tot_in_time_b3.to_numpy(),\n",
    "}\n",
    "\n",
    "df_in_time = pd.DataFrame(data=d)\n",
    "del d, selected_df, tdc_filtered_df\n",
    "\n",
    "df_in_time.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Time Walk Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_toa_b0 = (0.5*(df_in_time['toa_b1'] + df_in_time['toa_b3']) - df_in_time['toa_b0']).values\n",
    "del_toa_b1 = (0.5*(df_in_time['toa_b0'] + df_in_time['toa_b3']) - df_in_time['toa_b1']).values\n",
    "del_toa_b3 = (0.5*(df_in_time['toa_b0'] + df_in_time['toa_b1']) - df_in_time['toa_b3']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_b0 = np.polyfit(df_in_time['tot_b0'].values, del_toa_b0, 3)\n",
    "poly_func_b0 = np.poly1d(coeff_b0)\n",
    "\n",
    "coeff_b1 = np.polyfit(df_in_time['tot_b1'].values, del_toa_b1, 3)\n",
    "poly_func_b1 = np.poly1d(coeff_b1)\n",
    "\n",
    "coeff_b3 = np.polyfit(df_in_time['tot_b3'].values, del_toa_b3, 3)\n",
    "poly_func_b3 = np.poly1d(coeff_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[0].scatter(df_in_time['tot_b0'].values,  del_toa_b0, label='data')\n",
    "axes[0].plot(df_in_time['tot_b0'].values, poly_func_b0(df_in_time['tot_b0'].values), 'r.', label='fit')\n",
    "axes[0].set_xlabel('TOT time [ns]')\n",
    "axes[0].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]' )\n",
    "axes[0].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)    \n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[1].scatter(df_in_time['tot_b1'].values,  del_toa_b1, label='data')\n",
    "axes[1].plot(df_in_time['tot_b1'].values, poly_func_b1(df_in_time['tot_b1'].values), 'r.', label='fit')\n",
    "axes[1].set_xlabel('TOT time [ns]')\n",
    "axes[1].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[1].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[2].scatter(df_in_time['tot_b3'].values,  del_toa_b3, label='data')\n",
    "axes[2].plot(df_in_time['tot_b3'].values, poly_func_b3(df_in_time['tot_b3'].values), 'r.', label='fit')\n",
    "axes[2].set_xlabel('TOT time [ns]')\n",
    "axes[2].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_3d_fit(x, y, a, b, c, d, e, f, g, h, i, j):\n",
    "    return a*x**3 + b*y**3 + c*x**2*y + d*x*y**2 + e*x**2 + f*y**2 + g*x*y + h*x + i*y + j\n",
    "\n",
    "def _poly_3d_fit(M, *args):\n",
    "    x, y = M\n",
    "    arr = np.zeros(x.shape)\n",
    "    for i in range(len(args)//10):\n",
    "       arr += poly_3d_fit(x, y, *args[i*10:i*10+10])\n",
    "    return arr\n",
    "\n",
    "def get_basis(x, y, max_order=3):\n",
    "    \"\"\"Return the fit basis polynomials: 1, x, x^2, ..., xy, x^2y, ... etc.\"\"\"\n",
    "    basis = []\n",
    "    for i in range(max_order+1):\n",
    "        for j in range(max_order - i +1):\n",
    "            basis.append(x**j * y**i)\n",
    "    return basis\n",
    "\n",
    "X = df_in_time['tot_b0'].ravel()\n",
    "Y = df_in_time['tot_b1'].ravel()\n",
    "Z = (df_in_time['toa_b0'] - df_in_time['toa_b1']).ravel()\n",
    "\n",
    "xmin = X.min()\n",
    "xmax = X.max()\n",
    "ymin = Y.min()\n",
    "ymax = Y.max()\n",
    "nx = 100\n",
    "ny = 100\n",
    "\n",
    "x, y = np.linspace(xmin, xmax, nx), np.linspace(ymin, ymax, ny)\n",
    "\n",
    "max_order = 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = get_basis(X, Y, max_order)\n",
    "A = np.vstack(basis).T\n",
    "b = Z.ravel()\n",
    "c, r, rank, s = np.linalg.lstsq(A, b, rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitted parameters:')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fitted surface from the coefficients, c.\n",
    "fit = np.sum(c[:, None, None] * np.array(basis)\n",
    "                .reshape(len(basis), *X.shape), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit\n",
    "c[:, None, None]\n",
    "c[:, None, None]* np.array(basis)\n",
    "basis\n",
    "np.array(basis)\n",
    "c[:, None, None]* np.array(basis)\n",
    "(c[:, None, None]* np.array(basis)).shape\n",
    "c[:, None, None]* np.array(basis)\n",
    "fit = np.sum(c[:, None, None] * np.array(basis)\n",
    "                .reshape(len(basis), *X.shape), axis=0)\n",
    "fit\n",
    "np.array(basis)\n",
    "c[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[:, None, None]* np.array(basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c[:, None, None]* np.array(basis)\n",
    "  .reshape(len(basis), *X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3D figure of the fitted function and the residuals.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "meshX, meshY = np.meshgrid(X, Y)\n",
    "ax.plot_surface(meshX, meshY, fit, cmap='viridis')\n",
    "#cset = ax.contourf(X, Y, Z-fit, zdir='z', offset=-4, cmap='viridis')\n",
    "ax.set_zlim(-4,np.max(fit))\n",
    "plt.show()\n",
    "\n",
    "# Plot the test data as a 2D image and the fit as overlaid contours.\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(Z, origin='lower', cmap='viridis',\n",
    "#           extent=(x.min(), x.max(), y.min(), y.max()))\n",
    "# ax.contour(X, Y, fit, colors='w')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_toas = helper.iterative_timewalk_correction(df_in_time, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_del_toa_b0 = (0.5*(corr_toas[1] + corr_toas[2]) - corr_toas[0])\n",
    "nth_del_toa_b1 = (0.5*(corr_toas[0] + corr_toas[2]) - corr_toas[1])\n",
    "nth_del_toa_b3 = (0.5*(corr_toas[0] + corr_toas[1]) - corr_toas[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_b0 = np.polyfit(df_in_time['tot_b0'].values, nth_del_toa_b0, 3)\n",
    "poly_func_b0 = np.poly1d(coeff_b0)\n",
    "\n",
    "coeff_b1 = np.polyfit(df_in_time['tot_b1'].values, nth_del_toa_b1, 3)\n",
    "poly_func_b1 = np.poly1d(coeff_b1)\n",
    "\n",
    "coeff_b3 = np.polyfit(df_in_time['tot_b3'].values, nth_del_toa_b3, 3)\n",
    "poly_func_b3 = np.poly1d(coeff_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[0].scatter(df_in_time['tot_b0'].values,  nth_del_toa_b0, label='data')\n",
    "axes[0].plot(df_in_time['tot_b0'].values, poly_func_b0(df_in_time['tot_b0'].values), 'r.', label='fit')\n",
    "axes[0].set_xlabel('TOT time [ns]')\n",
    "axes[0].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]' )\n",
    "axes[0].legend() \n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)    \n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[1].scatter(df_in_time['tot_b1'].values,  nth_del_toa_b1, label='data')\n",
    "axes[1].plot(df_in_time['tot_b1'].values, poly_func_b1(df_in_time['tot_b1'].values), 'r.', label='fit')\n",
    "axes[1].set_xlabel('TOT time [ns]')\n",
    "axes[1].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[1].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[2].scatter(df_in_time['tot_b3'].values,  nth_del_toa_b3, label='data')\n",
    "axes[2].plot(df_in_time['tot_b3'].values, poly_func_b3(df_in_time['tot_b3'].values), 'r.', label='fit')\n",
    "axes[2].set_xlabel('TOT time [ns]')\n",
    "axes[2].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrTOA_b0 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "corrTOA_b1 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "corrTOA_b3 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "corrTOA_b0.fill(corr_toas[0])\n",
    "corrTOA_b1.fill(corr_toas[1])\n",
    "corrTOA_b3.fill(corr_toas[2])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b0, ax=axes[0], lw=2)\n",
    "axes[0].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[0].set_ylabel('Entries')\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)\n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b1, ax=axes[1], lw=2)\n",
    "axes[1].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[1].set_ylabel('Entries')\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b3, ax=axes[2], lw=2)\n",
    "axes[2].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[2].set_ylabel('Entries')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict = {\n",
    "    'evt': df_in_time['evt'].values,\n",
    "    'corr_toa_b0': corr_toas[0],\n",
    "    'corr_toa_b1': corr_toas[1],\n",
    "    'corr_toa_b3': corr_toas[2],\n",
    "}\n",
    "\n",
    "df_in_time_corr = pd.DataFrame(tmp_dict)\n",
    "del tmp_dict\n",
    "df_in_time_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_b01 = df_in_time_corr['corr_toa_b0'] - df_in_time_corr['corr_toa_b1']\n",
    "diff_b03 = df_in_time_corr['corr_toa_b0'] - df_in_time_corr['corr_toa_b3']\n",
    "diff_b13 = df_in_time_corr['corr_toa_b1'] - df_in_time_corr['corr_toa_b3']\n",
    "\n",
    "dTOA_b01 = hist.Hist(hist.axis.Regular(80, diff_b01.mean().round(2)-0.8, diff_b01.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_b03 = hist.Hist(hist.axis.Regular(80, diff_b03.mean().round(2)-0.8, diff_b03.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_b13 = hist.Hist(hist.axis.Regular(80, diff_b13.mean().round(2)-0.8, diff_b13.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_b01.fill(diff_b01)\n",
    "dTOA_b03.fill(diff_b03)\n",
    "dTOA_b13.fill(diff_b13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit using lmfit ExponentialGaussianModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b01, dTOA_b01, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b03, dTOA_b03, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b13, dTOA_b13, std_range_cut=0.4, width_factor=1, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0,err_b0 = helper.return_resolution_ps(fit_params_lmfit[0][0], fit_params_lmfit[0][1],\n",
    "                                            fit_params_lmfit[1][0], fit_params_lmfit[1][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1])\n",
    "res_b1,err_b1 = helper.return_resolution_ps(fit_params_lmfit[0][0], fit_params_lmfit[0][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1],\n",
    "                                            fit_params_lmfit[1][0], fit_params_lmfit[1][1])\n",
    "res_b3,err_b3 = helper.return_resolution_ps(fit_params_lmfit[1][0], fit_params_lmfit[1][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1],\n",
    "                                            fit_params_lmfit[0][0], fit_params_lmfit[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0:.2f} ps, error: {err_b0:.2f} ps')\n",
    "print(f'Board 1: {res_b1:.2f} ps, error: {err_b1:.2f} ps')\n",
    "print(f'Board 3: {res_b3:.2f} ps, error: {err_b3:.2f} ps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Time Walk Correction -  Pairwise Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dense_model(numpars=2, print_summary=False):\n",
    "    input  = Input(shape=(numpars,), name='input')\n",
    "    dense1 = Dense(8, activation='relu', name='dense1',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(input)\n",
    "    # dense2 = Dense(4, activation='relu', name='dense2',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense1)\n",
    "    output = Dense(1, activation='linear', name='output',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense1)\n",
    "    model  = Model(inputs=[input], outputs=output, name=\"simple_dense_NN\")\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    if(print_summary): print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = return_dense_model(print_summary=True)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_count = 10\n",
    "data_tag = 'tb_sep28_offset24_triggeroffset6'\n",
    "extra_tag = \"CALNarrow_TOA275to350forWB_NoTOTcut\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etroc_regression_using_NN(\n",
    "        input_df: pd.DataFrame,\n",
    "        variables: list,\n",
    "        data_tag: str,\n",
    "        extra_tag: str,\n",
    "        board_tag: str,\n",
    "        ensemble_count: int,\n",
    "        figure_title: str,\n",
    "        do_plotting: bool,\n",
    "    ):\n",
    "    filename = f'{data_tag}_weights_{extra_tag}_{board_tag}'\n",
    "\n",
    "    for en_idx in range(ensemble_count):\n",
    "        model = return_dense_model(numpars=2)\n",
    "        checkpointer = ModelCheckpoint(f'models/NNRun{en_idx}_{filename}.hdf5', verbose=0, save_best_only=True, monitor=\"val_loss\")\n",
    "        term = tf.keras.callbacks.TerminateOnNaN()\n",
    "        escb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=0)\n",
    "\n",
    "        shuffled_df = input_df.sample(frac=1)\n",
    "        \n",
    "        history = model.fit(\n",
    "            shuffled_df[[variables[0], variables[1]]].values, \n",
    "            (shuffled_df[variables[2]]-shuffled_df[variables[3]]).values, \n",
    "            validation_split=0.3, \n",
    "            epochs=150,\n",
    "            callbacks=[checkpointer,term,escb],\n",
    "            verbose=0)\n",
    "        \n",
    "        del model\n",
    "\n",
    "        if (do_plotting):\n",
    "            #plot the loss and validation loss of the dataset\n",
    "            fig, ax = plt.subplots(figsize=(15, 10), dpi=50)\n",
    "            hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "            ax.set_title(f'Model {en_idx}, {figure_title}', loc=\"right\", size=25)\n",
    "            plt.plot(history.history['loss'], label='Train data')\n",
    "            plt.plot(history.history['val_loss'], label='Validation data')\n",
    "            plt.ylabel('Loss (MSE)')\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"models/NNRun{en_idx}_{filename}.png\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NN Training (skip if you don't want to do training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['tot_b0', 'tot_b1', 'toa_b0', 'toa_b1']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b01\", ensemble_count, figure_title='Board 0 - Board 1', do_plotting=True)\n",
    "\n",
    "vars = ['tot_b0', 'tot_b3', 'toa_b0', 'toa_b3']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b03\", ensemble_count, figure_title='Board 0 - Board 3', do_plotting=True)\n",
    "\n",
    "vars = ['tot_b1', 'tot_b3', 'toa_b1', 'toa_b3']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b13\", ensemble_count, figure_title='Board 1 - Board 3', do_plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b01 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b01'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b01.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b01.predict(df_in_time[['tot_b0', 'tot_b1']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b01.predict(df_in_time[['tot_b0', 'tot_b1']].values, verbose=0).flatten()\n",
    "del model_b01\n",
    "Y_pred_b01 = Y_pred/ensemble_count\n",
    "\n",
    "model_b03 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b03'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b03.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b03.predict(df_in_time[['tot_b0', 'tot_b3']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b03.predict(df_in_time[['tot_b0', 'tot_b3']].values, verbose=0).flatten()\n",
    "del model_b03\n",
    "Y_pred_b03 = Y_pred/ensemble_count\n",
    "\n",
    "model_b13 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b13'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b13.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b13.predict(df_in_time[['tot_b1', 'tot_b3']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b13.predict(df_in_time[['tot_b1',  'tot_b3']].values, verbose=0).flatten()\n",
    "del model_b13\n",
    "Y_pred_b13 = Y_pred/ensemble_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b01 = (df_in_time['toa_b0']-df_in_time['toa_b1']).values-Y_pred_b01\n",
    "data_b03 = (df_in_time['toa_b0']-df_in_time['toa_b3']).values-Y_pred_b03\n",
    "data_b13 = (df_in_time['toa_b1']-df_in_time['toa_b3']).values-Y_pred_b13\n",
    "\n",
    "dTOA_NN_b01 = hist.Hist(hist.axis.Regular(50, data_b01.mean().round(2)-0.5, data_b01.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b03 = hist.Hist(hist.axis.Regular(50, data_b03.mean().round(2)-0.5, data_b03.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b13 = hist.Hist(hist.axis.Regular(50, data_b13.mean().round(2)-0.5, data_b13.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_NN_b01.fill(data_b01)\n",
    "dTOA_NN_b03.fill(data_b03)\n",
    "dTOA_NN_b13.fill(data_b13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit using lmfit ExponentialGaussianModel with NN outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit_NN = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b01, dTOA_NN_b01, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b03, dTOA_NN_b03, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b13, dTOA_NN_b13, std_range_cut=0.4, width_factor=1, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0_NN,err_b0_NN = helper.return_resolution_ps(fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1],\n",
    "                                            fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1])\n",
    "res_b1_NN,err_b1_NN = helper.return_resolution_ps(fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1],\n",
    "                                            fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1])\n",
    "res_b3_NN,err_b3_NN = helper.return_resolution_ps(fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1],\n",
    "                                            fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0_NN:.2f} ps, error: {err_b0_NN:.2f} ps')\n",
    "print(f'Board 1: {res_b1_NN:.2f} ps, error: {err_b1_NN:.2f} ps')\n",
    "print(f'Board 3: {res_b3_NN:.2f} ps, error: {err_b3_NN:.2f} ps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Top', 'Middle', 'Bottom']\n",
    "\n",
    "board_resols = {\n",
    "    'Single board TWC': ((res_b1, res_b0, res_b3), (err_b1, err_b0, err_b3)),\n",
    "    'Pairwise TWC': ((res_b1_NN, res_b0_NN, res_b3_NN), (err_b1_NN, err_b0_NN, err_b3_NN))\n",
    "}\n",
    "\n",
    "x = np.arange(len(names))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6), layout='constrained')\n",
    "\n",
    "for attribute, measurement in board_resols.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.barh(x + offset, measurement[0], width, edgecolor='white', xerr=measurement[1], label=attribute)\n",
    "    multiplier += 1\n",
    "\n",
    "rectangle = mpatch.Rectangle((55, 0), 40, 0.7, edgecolor='black', facecolor=\"none\", linewidth=1.5)\n",
    "ax.add_patch(rectangle)\n",
    "rx, ry = rectangle.get_xy()\n",
    "cx = rx + rectangle.get_width()/2.0\n",
    "cy = ry + rectangle.get_height()/2.\n",
    "ax.annotate(\"Top: FFF corner, Wire bonded 2x2, W36-IP7-13 (HPK, split3) \\n Middle: FFF corner, Bump bonded 16x16, W13 4-5 (FBK) \\n Bottom: FFF corner, Bump bonded 16x16, W16 P6 (HPK)\", (cx, cy), color='black', weight='bold', fontsize=13, ha='center', va='center')\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "ax.set_title(rf'TOA$_{{{names[0]}}} \\in \\left[275, 350\\right]$', loc=\"right\", size=20)\n",
    "ax.set_xlabel('Time Resolution [ps]', fontsize=20)  \n",
    "ax.set_yticks(x + width, names)\n",
    "ax.legend(loc='lower right', fontsize=18)\n",
    "ax.set_xlim(30, 100)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
