{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# zlib License\n",
    "#\n",
    "# (C) 2023 Murtaza Safdari <musafdar@cern.ch>, Jongho Lee <jongho.lee@cern.ch>\n",
    "#\n",
    "# This software is provided 'as-is', without any express or implied\n",
    "# warranty.  In no event will the authors be held liable for any damages\n",
    "# arising from the use of this software.\n",
    "#\n",
    "# Permission is granted to anyone to use this software for any purpose,\n",
    "# including commercial applications, and to alter it and redistribute it\n",
    "# freely, subject to the following restrictions:\n",
    "#\n",
    "# 1. The origin of this software must not be misrepresented; you must not\n",
    "#    claim that you wrote the original software. If you use this software\n",
    "#    in a product, an acknowledgment in the product documentation would be\n",
    "#    appreciated but is not required.\n",
    "# 2. Altered source versions must be plainly marked as such, and must not be\n",
    "#    misrepresented as being the original software.\n",
    "# 3. This notice may not be removed or altered from any source distribution.\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beamtest_analysis_helper as helper\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import hist\n",
    "import mplhep as hep\n",
    "hep.style.use('CMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!\n",
    "# It is very important to correctly set the chip name, this value is stored with the data\n",
    "\n",
    "chip_names = [\"ET2_W36_IP7_13_HV210V_offset24\",\"ET2_EPIR_1_1_HV210V_offset24\", \"ET2_CNM_1_3_HV210V_offset6\"]\n",
    "chip_fignames = chip_names\n",
    "chip_figtitles = [\"ETROC2 WB W36 IP7-13 HV210V OS:24\",\"(Trigger) ETROC2 BB EPIR 1-1 HV210V OS:24\", \"ETROC2 BB CNM 1-3 HV210V OS:6\"]\n",
    "\n",
    "chip_labels= [\"1\",\"0\",\"3\"]\n",
    "\n",
    "today = datetime.date.today().isoformat()\n",
    "fig_outdir = Path('../../ETROC-figures')\n",
    "fig_outdir = fig_outdir / (today + '_Array_Test_Results')\n",
    "fig_outdir.mkdir(exist_ok=True)\n",
    "fig_path = str(fig_outdir)\n",
    "\n",
    "# path_pattern = f\"*2023-09-21_Array_Test_Results/SelfTrigger_bottom_Readout_topbottom_1\"\n",
    "# path_pattern = f\"./testbeam_sep24/SelfTrigger_ET2_CNM_BATCH_1_3_Readout_ET2_EPIR_BATCH1_1_ET2_W36_IP7_13_ET2_CNM_BATCH1_3_loop_*.pqt\"\n",
    "path_pattern = \"./highpower_offset6/SelfTrigger_ET2_EPIR_BATCH1_1_Readout_ET2_W36_IP7_13_ET2_CNM_BATCH1_3_offset6_highpower_FINALRUN_loop_*.pqt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "\n",
    "fig = plt.figure(dpi=50, figsize=(5,5))\n",
    "gs = fig.add_gridspec(1,1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0,0])\n",
    "ax0.plot([1, 0], [1, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(path_pattern)\n",
    "files = natsorted(files)\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for ifile in files:\n",
    "    tmp_df = pd.read_parquet(ifile)\n",
    "    if tmp_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Group the DataFrame by 'evt' and count unique 'board' values in each group\n",
    "    unique_board_counts = tmp_df.groupby('evt')['board'].nunique()\n",
    "\n",
    "    ## event has three unique board ID\n",
    "    event_numbers_with_three_unique_boards = unique_board_counts[unique_board_counts == 3].index\n",
    "    subset_df = tmp_df[tmp_df['evt'].isin(event_numbers_with_three_unique_boards)]\n",
    "    subset_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    del tmp_df\n",
    "    if subset_df.empty:\n",
    "        continue\n",
    "\n",
    "    dataframes.append(subset_df)\n",
    "\n",
    "df = pd.concat(dataframes)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "del dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_inclusive = helper.return_hist(df, chip_names, chip_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_inclusive, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_inclusive, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_inclusive, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=\", inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.making_heatmap_byPandas(df, chip_labels, chip_figtitles, \"inclusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.making_3d_heatmap_byPandas(df, chip_labels, chip_figtitles, \"inclusive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event counter and 2D heatmap based on WB pixel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_info = [1, 15, 7]\n",
    "simple_filtered_df = helper.find_maximum_event_combination(df, pix_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.making_heatmap_byPandas(simple_filtered_df, chip_labels, chip_figtitles, figtitle_tag=\"WB pixel (15, 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del simple_filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel ID selection based on the event counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_dict = {\n",
    "    # board ID: [row, col]\n",
    "    0: [ 1, 6],\n",
    "    1: [15, 7],\n",
    "    2: [ 0, 0],\n",
    "    3: [ 2, 5],\n",
    "}\n",
    "\n",
    "filtered_group = helper.pixel_filter(df, pix_dict)\n",
    "filtered_group = helper.singlehit_event_clear_func(filtered_group)\n",
    "filtered_group.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pix_selected = helper.return_hist(filtered_group, chip_names, chip_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_pix_selected, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=f\", Pixel ({pix_dict[1][0]}, {pix_dict[1][1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_pix_selected, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=f\", Pixel ({pix_dict[0][0]}, {pix_dict[0][1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_pix_selected, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True, tag=\"inclusive\", title_tag=f\", Pixel ({pix_dict[3][0]}, {pix_dict[3][1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_pix_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDC cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom filtering criteria for each board\n",
    "tdc_cuts = {\n",
    "    # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "    0: [199, 205,   0, 1100,   0, 600],\n",
    "    1: [200, 210, 275,  350,   0, 600], # pixel (15, 6), (15, 7) Sep 28th data\n",
    "    # 1: [200, 210, 100,  500,   0, 600], # pixel (15, 6), (15, 7) Sep 28th data\n",
    "    2: [160, 220,   0, 1100,   0, 600],\n",
    "    3: [189, 195,   0, 1000,   0, 600],\n",
    "}\n",
    "# tdc_cuts = {\n",
    "#     # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "#     0: [200, 210,   0, 1100,   0, 600],\n",
    "#     # 1: [173, 180, 250,  325,   0, 600], # pixel (15, 8) Sep 28th data\n",
    "#     # 1: [173, 180, 262,  337,   0, 600], # pixel (15, 9) Sep 28th data\n",
    "#     1: [173, 180, 100,  500,   0, 600], \n",
    "#     2: [160, 220,   0, 1100,   0, 600],\n",
    "#     3: [189, 195,   0, 1000,   0, 600], # pixel ()\n",
    "# }\n",
    "\n",
    "tdc_filtered_df = helper.tdc_event_selection(filtered_group, tdc_cuts)\n",
    "tdc_filtered_df = helper.singlehit_event_clear_func(tdc_filtered_df)\n",
    "tdc_filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tdc_selection = helper.return_hist(tdc_filtered_df, chip_names, chip_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_tdc_selection, chip_names[0], chip_fignames[0], chip_figtitles[0], fig_path, save=False, show=True, tag=\"after_tdc_cut\", title_tag=f\", Pixel ({pix_dict[1][0]}, {pix_dict[1][1]}) after TDC cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_tdc_selection, chip_names[1], chip_fignames[1], chip_figtitles[1], fig_path, save=False, show=True, tag=\"\", title_tag=f\", Pixel ({pix_dict[0][0]}, {pix_dict[0][1]}) after TDC cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.make_pix_inclusive_plots(h_tdc_selection, chip_names[2], chip_fignames[2], chip_figtitles[2], fig_path, save=False, show=True, tag=\"\", title_tag=f\", Pixel ({pix_dict[3][0]}, {pix_dict[3][1]}) after TDC cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h_tdc_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert TDC code to TDC time in [ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = tdc_filtered_df\n",
    "\n",
    "pix_rows = []\n",
    "pix_cols = []\n",
    "fit_params = []\n",
    "cal_means = {boardID:{} for boardID in chip_labels}\n",
    "\n",
    "for boardID in chip_labels:\n",
    "    groups = selected_df[selected_df['board'] == int(boardID)].groupby(['row', 'col'])\n",
    "    for (row, col), group in groups:\n",
    "        \n",
    "        cal_mean = group['cal'].mean()\n",
    "        cal_means[boardID][(row, col)] = cal_mean\n",
    "\n",
    "cal_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin0 = (3.125/cal_means[\"0\"][(pix_dict[0][0], pix_dict[0][1])])\n",
    "bin1 = (3.125/cal_means[\"1\"][(pix_dict[1][0], pix_dict[1][1])])\n",
    "bin3 = (3.125/cal_means[\"3\"][(pix_dict[3][0], pix_dict[3][1])])\n",
    "\n",
    "toa_in_time_b0 = 12.5 - selected_df[selected_df['board'] == 0]['toa'] * bin0\n",
    "toa_in_time_b1 = 12.5 - selected_df[selected_df['board'] == 1]['toa'] * bin1\n",
    "toa_in_time_b3 = 12.5 - selected_df[selected_df['board'] == 3]['toa'] * bin3\n",
    "\n",
    "tot_in_time_b0 = (2*selected_df[selected_df['board'] == 0]['tot'] - np.floor(selected_df[selected_df['board'] == 0]['tot']/32)) * bin0\n",
    "tot_in_time_b1 = (2*selected_df[selected_df['board'] == 1]['tot'] - np.floor(selected_df[selected_df['board'] == 1]['tot']/32)) * bin1\n",
    "tot_in_time_b3 = (2*selected_df[selected_df['board'] == 3]['tot'] - np.floor(selected_df[selected_df['board'] == 3]['tot']/32)) * bin3\n",
    "\n",
    "d = {\n",
    "    'evt': selected_df['evt'].unique(),\n",
    "    'toa_b0': toa_in_time_b0.to_numpy(),\n",
    "    'tot_b0': tot_in_time_b0.to_numpy(),\n",
    "    'toa_b1': toa_in_time_b1.to_numpy(),\n",
    "    'tot_b1': tot_in_time_b1.to_numpy(),\n",
    "    'toa_b3': toa_in_time_b3.to_numpy(),\n",
    "    'tot_b3': tot_in_time_b3.to_numpy(),\n",
    "}\n",
    "\n",
    "df_in_time = pd.DataFrame(data=d)\n",
    "del d, selected_df, tdc_filtered_df\n",
    "\n",
    "df_in_time.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Time Walk Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_toa_b0 = (0.5*(df_in_time['toa_b1'] + df_in_time['toa_b3']) - df_in_time['toa_b0']).values\n",
    "del_toa_b1 = (0.5*(df_in_time['toa_b0'] + df_in_time['toa_b3']) - df_in_time['toa_b1']).values\n",
    "del_toa_b3 = (0.5*(df_in_time['toa_b0'] + df_in_time['toa_b1']) - df_in_time['toa_b3']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_b0 = np.polyfit(df_in_time['tot_b0'].values, del_toa_b0, 3)\n",
    "poly_func_b0 = np.poly1d(coeff_b0)\n",
    "\n",
    "coeff_b1 = np.polyfit(df_in_time['tot_b1'].values, del_toa_b1, 3)\n",
    "poly_func_b1 = np.poly1d(coeff_b1)\n",
    "\n",
    "coeff_b3 = np.polyfit(df_in_time['tot_b3'].values, del_toa_b3, 3)\n",
    "poly_func_b3 = np.poly1d(coeff_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[0].scatter(df_in_time['tot_b0'].values,  del_toa_b0, label='data')\n",
    "axes[0].plot(df_in_time['tot_b0'].values, poly_func_b0(df_in_time['tot_b0'].values), 'r.', label='fit')\n",
    "axes[0].set_xlabel('TOT time [ns]')\n",
    "axes[0].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]' )\n",
    "axes[0].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)    \n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[1].scatter(df_in_time['tot_b1'].values,  del_toa_b1, label='data')\n",
    "axes[1].plot(df_in_time['tot_b1'].values, poly_func_b1(df_in_time['tot_b1'].values), 'r.', label='fit')\n",
    "axes[1].set_xlabel('TOT time [ns]')\n",
    "axes[1].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[1].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[2].scatter(df_in_time['tot_b3'].values,  del_toa_b3, label='data')\n",
    "axes[2].plot(df_in_time['tot_b3'].values, poly_func_b3(df_in_time['tot_b3'].values), 'r.', label='fit')\n",
    "axes[2].set_xlabel('TOT time [ns]')\n",
    "axes[2].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_toas = helper.iterative_timewalk_correction(df_in_time, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_del_toa_b0 = (0.5*(corr_toas[1] + corr_toas[2]) - corr_toas[0])\n",
    "nth_del_toa_b1 = (0.5*(corr_toas[0] + corr_toas[2]) - corr_toas[1])\n",
    "nth_del_toa_b3 = (0.5*(corr_toas[0] + corr_toas[1]) - corr_toas[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_b0 = np.polyfit(df_in_time['tot_b0'].values, nth_del_toa_b0, 3)\n",
    "poly_func_b0 = np.poly1d(coeff_b0)\n",
    "\n",
    "coeff_b1 = np.polyfit(df_in_time['tot_b1'].values, nth_del_toa_b1, 3)\n",
    "poly_func_b1 = np.poly1d(coeff_b1)\n",
    "\n",
    "coeff_b3 = np.polyfit(df_in_time['tot_b3'].values, nth_del_toa_b3, 3)\n",
    "poly_func_b3 = np.poly1d(coeff_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[0].scatter(df_in_time['tot_b0'].values,  nth_del_toa_b0, label='data')\n",
    "axes[0].plot(df_in_time['tot_b0'].values, poly_func_b0(df_in_time['tot_b0'].values), 'r.', label='fit')\n",
    "axes[0].set_xlabel('TOT time [ns]')\n",
    "axes[0].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]' )\n",
    "axes[0].legend() \n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)    \n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[1].scatter(df_in_time['tot_b1'].values,  nth_del_toa_b1, label='data')\n",
    "axes[1].plot(df_in_time['tot_b1'].values, poly_func_b1(df_in_time['tot_b1'].values), 'r.', label='fit')\n",
    "axes[1].set_xlabel('TOT time [ns]')\n",
    "axes[1].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[1].legend()\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "axes[2].scatter(df_in_time['tot_b3'].values,  nth_del_toa_b3, label='data')\n",
    "axes[2].plot(df_in_time['tot_b3'].values, poly_func_b3(df_in_time['tot_b3'].values), 'r.', label='fit')\n",
    "axes[2].set_xlabel('TOT time [ns]')\n",
    "axes[2].set_ylabel(r'$(TOA_{i} + TOA_{j})/2 - TOA$ [ns]')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrTOA_b0 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "corrTOA_b1 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "corrTOA_b3 = hist.Hist(hist.axis.Regular(60, 0, 12, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "corrTOA_b0.fill(corr_toas[0])\n",
    "corrTOA_b1.fill(corr_toas[1])\n",
    "corrTOA_b3.fill(corr_toas[2])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "hep.cms.text(loc=0, ax=axes[0], text=\"Preliminary\", fontsize=25)\n",
    "axes[0].set_title(f'Board 0 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b0, ax=axes[0], lw=2)\n",
    "axes[0].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[0].set_ylabel('Entries')\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[1], text=\"Preliminary\", fontsize=25)\n",
    "axes[1].set_title(f'Board 1 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b1, ax=axes[1], lw=2)\n",
    "axes[1].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[1].set_ylabel('Entries')\n",
    "\n",
    "hep.cms.text(loc=0, ax=axes[2], text=\"Preliminary\", fontsize=25)\n",
    "axes[2].set_title(f'Board 3 Time Walk Correction', loc=\"right\", size=25)\n",
    "hep.histplot(corrTOA_b3, ax=axes[2], lw=2)\n",
    "axes[2].set_xlabel('Time Walk Corrected TOA [ns]')\n",
    "axes[2].set_ylabel('Entries')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict = {\n",
    "    'evt': df_in_time['evt'].values,\n",
    "    'corr_toa_b0': corr_toas[0],\n",
    "    'corr_toa_b1': corr_toas[1],\n",
    "    'corr_toa_b3': corr_toas[2],\n",
    "}\n",
    "\n",
    "df_in_time_corr = pd.DataFrame(tmp_dict)\n",
    "del tmp_dict\n",
    "df_in_time_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_b01 = df_in_time_corr['corr_toa_b0'] - df_in_time_corr['corr_toa_b1']\n",
    "diff_b03 = df_in_time_corr['corr_toa_b0'] - df_in_time_corr['corr_toa_b3']\n",
    "diff_b13 = df_in_time_corr['corr_toa_b1'] - df_in_time_corr['corr_toa_b3']\n",
    "\n",
    "dTOA_b01 = hist.Hist(hist.axis.Regular(80, diff_b01.mean().round(2)-0.8, diff_b01.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_b03 = hist.Hist(hist.axis.Regular(80, diff_b03.mean().round(2)-0.8, diff_b03.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_b13 = hist.Hist(hist.axis.Regular(80, diff_b13.mean().round(2)-0.8, diff_b13.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_b01.fill(diff_b01)\n",
    "dTOA_b03.fill(diff_b03)\n",
    "dTOA_b13.fill(diff_b13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit using lmfit GaussianModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b01, dTOA_b01, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b03, dTOA_b03, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(diff_b13, dTOA_b13, std_range_cut=0.4, width_factor=1.25, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0,err_b0 = helper.return_resolution_ps(fit_params_lmfit[0][0], fit_params_lmfit[0][1],\n",
    "                                            fit_params_lmfit[1][0], fit_params_lmfit[1][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1])\n",
    "res_b1,err_b1 = helper.return_resolution_ps(fit_params_lmfit[0][0], fit_params_lmfit[0][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1],\n",
    "                                            fit_params_lmfit[1][0], fit_params_lmfit[1][1])\n",
    "res_b3,err_b3 = helper.return_resolution_ps(fit_params_lmfit[1][0], fit_params_lmfit[1][1],\n",
    "                                            fit_params_lmfit[2][0], fit_params_lmfit[2][1],\n",
    "                                            fit_params_lmfit[0][0], fit_params_lmfit[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0:.2f} ps, error: {err_b0:.2f} ps')\n",
    "print(f'Board 1: {res_b1:.2f} ps, error: {err_b1:.2f} ps')\n",
    "print(f'Board 3: {res_b3:.2f} ps, error: {err_b3:.2f} ps')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-polynomial fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly2D_2_fit(M, *args):\n",
    "    x, y = M\n",
    "    return helper.poly2D(2, x, y, *args)\n",
    "\n",
    "def poly2D_3_fit(M, *args):\n",
    "    x, y = M\n",
    "    return helper.poly2D(3, x, y, *args)\n",
    "\n",
    "def poly2D_4_fit(M, *args):\n",
    "    x, y = M\n",
    "    return helper.poly2D(4, x, y, *args)\n",
    "\n",
    "def poly3D_2_fit(M, *args):\n",
    "    x, y, z = M\n",
    "    return helper.poly3D(2, x, y, z, *args)\n",
    "    \n",
    "helper.poly2D(0, df_in_time['tot_b0'], df_in_time['tot_b1'], 2)                              #  pol: z = 2\n",
    "helper.poly2D(1, df_in_time['tot_b0'], df_in_time['tot_b1'], 2, 4, 1)                        #  pol: z = 2 + 4x + y\n",
    "helper.poly2D(2, df_in_time['tot_b0'], df_in_time['tot_b1'], 2, 0.2, -0.9, 0.3, -0.1, 0.01)  #  pol: z = 2 + 0.2x - 0.9x^2 + 0.3y - 0.1xy + 0.01y^2\n",
    "\n",
    "import scipy.optimize as opt\n",
    "x = df_in_time['tot_b0']\n",
    "y = df_in_time['tot_b1']\n",
    "z = df_in_time['tot_b3']\n",
    "data = df_in_time['toa_b0'] - df_in_time['toa_b1']\n",
    "data = df_in_time['toa_b0'] - df_in_time['toa_b3']\n",
    "data = df_in_time['toa_b1'] - df_in_time['toa_b3']\n",
    "initial_guess = (data.mean(), 0, 0, 0, 0, 0)\n",
    "popt, pcov = opt.curve_fit(poly2D_2_fit, (x,y), data, p0 = initial_guess)\n",
    "corr_data = helper.poly2D(2, df_in_time['tot_b0'], df_in_time['tot_b1'], *popt)\n",
    "\n",
    "initial_guess = (data.mean(), 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "popt, pcov = opt.curve_fit(poly2D_3_fit, (x,y), data, p0 = initial_guess)\n",
    "corr_data = helper.poly2D(3, df_in_time['tot_b0'], df_in_time['tot_b1'], *popt)\n",
    "\n",
    "initial_guess = (data.mean(), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "popt, pcov = opt.curve_fit(poly2D_4_fit, (x,y), data, p0 = initial_guess)\n",
    "corr_data = helper.poly2D(4, df_in_time['tot_b0'], df_in_time['tot_b1'], *popt)\n",
    "\n",
    "#initial_guess = (data.mean(), 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "#popt, pcov = opt.curve_fit(poly3D_2_fit, (x,y,z), data, p0 = initial_guess)\n",
    "#corr_data = poly3D(2, df_in_time['tot_b0'], df_in_time['tot_b1'], df_in_time['tot_b3'], *popt)\n",
    "\n",
    "dTOA_test = hist.Hist(hist.axis.Regular(80, corr_data.mean().round(2)-0.8, corr_data.mean().round(2)+0.8, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_test.fill(corr_data)\n",
    "dTOA_test\n",
    "\n",
    "params = helper.lmfit_gaussfit_with_pulls(corr_data, dTOA_test, std_range_cut=0.4, width_factor=1, fig_title='Board 1 - Board 3', use_pred_uncert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Time Walk Correction -  Pairwise Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dense_model(numpars=2, print_summary=False):\n",
    "    input  = Input(shape=(numpars,), name='input')\n",
    "    dense1 = Dense(8, activation='relu', name='dense1',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(input)\n",
    "    dense2 = Dense(8, activation='relu', name='dense2',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense1)\n",
    "    output = Dense(1, activation='linear', name='output',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense2)\n",
    "    model  = Model(inputs=[input], outputs=output, name=\"simple_dense_NN\")\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    if(print_summary): print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = return_dense_model(print_summary=True)\n",
    "del model\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "\n",
    "def return_combined_dense_model(numpars=3, print_summary=False):\n",
    "    input  = Input(shape=(numpars,), name='input')\n",
    "    dense1 = Dense(8, activation='relu', name='dense1',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(input)\n",
    "    dense2 = Dense(8, activation='relu', name='dense2',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense1)\n",
    "    output = Dense(3, activation='linear', name='output',kernel_initializer=initializers.RandomNormal(),bias_initializer=initializers.Zeros())(dense2)\n",
    "    model  = Model(inputs=[input], outputs=output, name=\"simple_dense_NN\")\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    if(print_summary): print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = return_combined_dense_model(print_summary=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etroc_regression_using_NN(\n",
    "        input_df: pd.DataFrame,\n",
    "        variables: list,\n",
    "        data_tag: str,\n",
    "        extra_tag: str,\n",
    "        board_tag: str,\n",
    "        ensemble_count: int,\n",
    "        figure_title: str,\n",
    "        do_plotting: bool,\n",
    "        epochs: int = 300,\n",
    "    ):\n",
    "    filename = f'{data_tag}_weights_{extra_tag}_{board_tag}'\n",
    "\n",
    "    for en_idx in range(ensemble_count):\n",
    "        model = return_dense_model(numpars=2)\n",
    "        checkpointer = ModelCheckpoint(f'models/NNRun{en_idx}_{filename}.hdf5', verbose=0, save_best_only=True, monitor=\"val_loss\")\n",
    "        term = tf.keras.callbacks.TerminateOnNaN()\n",
    "        escb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=0)\n",
    "\n",
    "        shuffled_df = input_df.sample(frac=1)\n",
    "        \n",
    "        history = model.fit(\n",
    "            shuffled_df[[variables[0], variables[1]]].values, \n",
    "            (shuffled_df[variables[2]]-shuffled_df[variables[3]]).values, \n",
    "            validation_split=0.3, \n",
    "            epochs=epochs,\n",
    "            callbacks=[checkpointer,term,escb],\n",
    "            verbose=0)\n",
    "        \n",
    "        del model\n",
    "\n",
    "        if (do_plotting):\n",
    "            #plot the loss and validation loss of the dataset\n",
    "            fig, ax = plt.subplots(figsize=(15, 10), dpi=50)\n",
    "            hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "            ax.set_title(f'Model {en_idx}, {figure_title}', loc=\"right\", size=25)\n",
    "            plt.plot(history.history['loss'], label='Train data')\n",
    "            plt.plot(history.history['val_loss'], label='Validation data')\n",
    "            plt.ylabel('Loss (MSE)')\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"models/NNRun{en_idx}_{filename}.png\")\n",
    "            plt.show()\n",
    "\n",
    "## --------------------------------------------------------------\n",
    "\n",
    "def etroc_regression_using_combined_NN(\n",
    "        input_df: pd.DataFrame,\n",
    "        variables: list,\n",
    "        data_tag: str,\n",
    "        extra_tag: str,\n",
    "        ensemble_count: int,\n",
    "        figure_title: str,\n",
    "        do_plotting: bool,\n",
    "        epochs: int = 300,\n",
    "    ):\n",
    "    filename = f'{data_tag}_weights_{extra_tag}'\n",
    "\n",
    "    inner_df = input_df.copy()\n",
    "    inner_df['delta01'] = inner_df[variables[3]] - inner_df[variables[4]]\n",
    "    inner_df['delta03'] = inner_df[variables[3]] - inner_df[variables[5]]\n",
    "    inner_df['delta13'] = inner_df[variables[4]] - inner_df[variables[5]]\n",
    "\n",
    "    for en_idx in range(ensemble_count):\n",
    "        model = return_combined_dense_model(numpars=3)\n",
    "        checkpointer = ModelCheckpoint(f'models/CombinedNNRun{en_idx}_{filename}.hdf5', verbose=0, save_best_only=True, monitor=\"val_loss\")\n",
    "        term = tf.keras.callbacks.TerminateOnNaN()\n",
    "        escb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, verbose=0)\n",
    "\n",
    "        shuffled_df = inner_df.sample(frac=1)\n",
    "        \n",
    "        history = model.fit(\n",
    "            shuffled_df[[variables[0], variables[1], variables[2]]].values, \n",
    "            shuffled_df[['delta01', 'delta03', 'delta13']].values, \n",
    "            validation_split=0.3, \n",
    "            epochs=epochs,\n",
    "            callbacks=[checkpointer,term,escb],\n",
    "            verbose=0)\n",
    "        \n",
    "        del model\n",
    "\n",
    "        if (do_plotting):\n",
    "            #plot the loss and validation loss of the dataset\n",
    "            fig, ax = plt.subplots(figsize=(15, 10), dpi=50)\n",
    "            hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "            ax.set_title(f'Model {en_idx}, {figure_title}', loc=\"right\", size=25)\n",
    "            plt.plot(history.history['loss'], label='Train data')\n",
    "            plt.plot(history.history['val_loss'], label='Validation data')\n",
    "            plt.ylabel('Loss (MSE)')\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"models/CombinedNNRun{en_idx}_{filename}.png\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_count = 10\n",
    "data_tag = 'tb_sep28_offset24_Bottom_offset6'\n",
    "\n",
    "# extra_tag = \"WB156_CALNarrow_TOA275to350forWB_NoTOTcut\"\n",
    "extra_tag = \"WB157_CALNarrow_TOA275to350forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB158_CALNarrow_TOA250to325forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB159_CALNarrow_TOA262to337forWB_NoTOTcut\"\n",
    "\n",
    "# extra_tag = \"WB156_CALNarrow_TOA100to500forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB157_CALNarrow_TOA100to500forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB158_CALNarrow_TOA100to500forWB_NoTOTcut\"\n",
    "# extra_tag = \"WB159_CALNarrow_TOA100to500forWB_NoTOTcut\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NN Training (skip if you don't want to do training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars = ['tot_b0', 'tot_b1', 'toa_b0', 'toa_b1']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b01\", ensemble_count, figure_title='Board 0 - Board 1', do_plotting=True)\n",
    "\n",
    "vars = ['tot_b0', 'tot_b3', 'toa_b0', 'toa_b3']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b03\", ensemble_count, figure_title='Board 0 - Board 3', do_plotting=True)\n",
    "\n",
    "vars = ['tot_b1', 'tot_b3', 'toa_b1', 'toa_b3']\n",
    "etroc_regression_using_NN(df_in_time, vars, data_tag, extra_tag, \"b13\", ensemble_count, figure_title='Board 1 - Board 3', do_plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['tot_b0', 'tot_b1', 'tot_b3', 'toa_b0', 'toa_b1', 'toa_b3']\n",
    "etroc_regression_using_combined_NN(df_in_time, vars, data_tag, extra_tag, ensemble_count, figure_title='Combined Loss', do_plotting=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load individual trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b01 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b01'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b01.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b01.predict(df_in_time[['tot_b0', 'tot_b1']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b01.predict(df_in_time[['tot_b0', 'tot_b1']].values, verbose=0).flatten()\n",
    "del model_b01\n",
    "Y_pred_b01 = Y_pred/ensemble_count\n",
    "\n",
    "model_b03 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b03'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b03.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b03.predict(df_in_time[['tot_b0', 'tot_b3']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b03.predict(df_in_time[['tot_b0', 'tot_b3']].values, verbose=0).flatten()\n",
    "del model_b03\n",
    "Y_pred_b03 = Y_pred/ensemble_count\n",
    "\n",
    "model_b13 = return_dense_model(numpars=2)\n",
    "filename = f'{data_tag}_weights_{extra_tag}_b13'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_b13.load_weights(f'models/NNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_b13.predict(df_in_time[['tot_b1', 'tot_b3']].values, verbose=0).flatten()\n",
    "    else: Y_pred += model_b13.predict(df_in_time[['tot_b1',  'tot_b3']].values, verbose=0).flatten()\n",
    "del model_b13\n",
    "Y_pred_b13 = Y_pred/ensemble_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b01 = (df_in_time['toa_b0']-df_in_time['toa_b1']).values-Y_pred_b01\n",
    "data_b03 = (df_in_time['toa_b0']-df_in_time['toa_b3']).values-Y_pred_b03\n",
    "data_b13 = (df_in_time['toa_b1']-df_in_time['toa_b3']).values-Y_pred_b13\n",
    "\n",
    "dTOA_NN_b01 = hist.Hist(hist.axis.Regular(50, data_b01.mean().round(2)-0.5, data_b01.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b03 = hist.Hist(hist.axis.Regular(50, data_b03.mean().round(2)-0.5, data_b03.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b13 = hist.Hist(hist.axis.Regular(50, data_b13.mean().round(2)-0.5, data_b13.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_NN_b01.fill(data_b01)\n",
    "dTOA_NN_b03.fill(data_b03)\n",
    "dTOA_NN_b13.fill(data_b13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load combined trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined = return_combined_dense_model(numpars=3)\n",
    "filename = f'{data_tag}_weights_{extra_tag}'\n",
    "for en_idx in range(ensemble_count):\n",
    "    model_combined.load_weights(f'models/CombinedNNRun{en_idx}_{filename}.hdf5')\n",
    "    if(en_idx==0): Y_pred = model_combined.predict(df_in_time[['tot_b0', 'tot_b1', 'tot_b3']].values, verbose=0)\n",
    "    else: Y_pred += model_combined.predict(df_in_time[['tot_b0', 'tot_b1', 'tot_b3']].values, verbose=0)\n",
    "del model_combined\n",
    "Y_pred_combined = Y_pred/ensemble_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_b01_combined = (df_in_time['toa_b0']-df_in_time['toa_b1']).values-Y_pred_combined[:,0]\n",
    "data_b03_combined = (df_in_time['toa_b0']-df_in_time['toa_b3']).values-Y_pred_combined[:,1]\n",
    "data_b13_combined = (df_in_time['toa_b1']-df_in_time['toa_b3']).values-Y_pred_combined[:,2]\n",
    "\n",
    "dTOA_NN_b01_combined = hist.Hist(hist.axis.Regular(50, data_b01_combined.mean().round(2)-0.5, data_b01_combined.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b03_combined = hist.Hist(hist.axis.Regular(50, data_b03_combined.mean().round(2)-0.5, data_b03_combined.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "dTOA_NN_b13_combined = hist.Hist(hist.axis.Regular(50, data_b13_combined.mean().round(2)-0.5, data_b13_combined.mean().round(2)+0.5, name=\"TWC_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "\n",
    "dTOA_NN_b01_combined.fill(data_b01_combined)\n",
    "dTOA_NN_b03_combined.fill(data_b03_combined)\n",
    "dTOA_NN_b13_combined.fill(data_b13_combined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit using lmfit GaussianModel with NN outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit_NN = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b01, dTOA_NN_b01, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b03, dTOA_NN_b03, std_range_cut=0.4, width_factor=1.25, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b13, dTOA_NN_b13, std_range_cut=0.4, width_factor=1.25, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params_lmfit_NN_combined = []\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b01_combined, dTOA_NN_b01_combined, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 1', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN_combined.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b03_combined, dTOA_NN_b03_combined, std_range_cut=0.4, width_factor=1, fig_title='Board 0 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN_combined.append(params)\n",
    "params = helper.lmfit_gaussfit_with_pulls(data_b13_combined, dTOA_NN_b13_combined, std_range_cut=0.4, width_factor=1, fig_title='Board 1 - Board 3', use_pred_uncert=True)\n",
    "fit_params_lmfit_NN_combined.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0_NN,err_b0_NN = helper.return_resolution_ps(fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1],\n",
    "                                            fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1])\n",
    "res_b1_NN,err_b1_NN = helper.return_resolution_ps(fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1],\n",
    "                                            fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1])\n",
    "res_b3_NN,err_b3_NN = helper.return_resolution_ps(fit_params_lmfit_NN[1][0], fit_params_lmfit_NN[1][1],\n",
    "                                            fit_params_lmfit_NN[2][0], fit_params_lmfit_NN[2][1],\n",
    "                                            fit_params_lmfit_NN[0][0], fit_params_lmfit_NN[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0_NN:.2f} ps, error: {err_b0_NN:.2f} ps')\n",
    "print(f'Board 1: {res_b1_NN:.2f} ps, error: {err_b1_NN:.2f} ps')\n",
    "print(f'Board 3: {res_b3_NN:.2f} ps, error: {err_b3_NN:.2f} ps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b0_NN_combined,err_b0_NN_combined = helper.return_resolution_ps(fit_params_lmfit_NN_combined[0][0], fit_params_lmfit_NN_combined[0][1],\n",
    "                                            fit_params_lmfit_NN_combined[1][0], fit_params_lmfit_NN_combined[1][1],\n",
    "                                            fit_params_lmfit_NN_combined[2][0], fit_params_lmfit_NN_combined[2][1])\n",
    "res_b1_NN_combined,err_b1_NN_combined = helper.return_resolution_ps(fit_params_lmfit_NN_combined[0][0], fit_params_lmfit_NN_combined[0][1],\n",
    "                                            fit_params_lmfit_NN_combined[2][0], fit_params_lmfit_NN_combined[2][1],\n",
    "                                            fit_params_lmfit_NN_combined[1][0], fit_params_lmfit_NN_combined[1][1])\n",
    "res_b3_NN_combined,err_b3_NN_combined = helper.return_resolution_ps(fit_params_lmfit_NN_combined[1][0], fit_params_lmfit_NN_combined[1][1],\n",
    "                                            fit_params_lmfit_NN_combined[2][0], fit_params_lmfit_NN_combined[2][1],\n",
    "                                            fit_params_lmfit_NN_combined[0][0], fit_params_lmfit_NN_combined[0][1])\n",
    "\n",
    "print(f'Board 0: {res_b0_NN_combined:.2f} ps, error: {err_b0_NN_combined:.2f} ps')\n",
    "print(f'Board 1: {res_b1_NN_combined:.2f} ps, error: {err_b1_NN_combined:.2f} ps')\n",
    "print(f'Board 3: {res_b3_NN_combined:.2f} ps, error: {err_b3_NN_combined:.2f} ps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Top', 'Middle', 'Bottom']\n",
    "\n",
    "board_resols = {\n",
    "    'Single board TWC': ((res_b1, res_b0, res_b3), (err_b1, err_b0, err_b3)),\n",
    "    'Pairwise TWC': ((res_b1_NN, res_b0_NN, res_b3_NN), (err_b1_NN, err_b0_NN, err_b3_NN)),\n",
    "    # 'Combined TWC': ((res_b1_NN_combined, res_b0_NN_combined, res_b3_NN_combined), (err_b1_NN_combined, err_b0_NN_combined, err_b3_NN_combined)),\n",
    "}\n",
    "\n",
    "x = np.arange(len(names))  # the label locations\n",
    "width = 0.1  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6), layout='constrained')\n",
    "\n",
    "for attribute, measurement in board_resols.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.barh(x + offset, measurement[0], width, edgecolor='white', xerr=measurement[1], label=attribute)\n",
    "    multiplier += 1\n",
    "\n",
    "rectangle = mpatch.Rectangle((55, 0.2), 40, 0.6, edgecolor='black', facecolor=\"none\", linewidth=1.5)\n",
    "ax.add_patch(rectangle)\n",
    "rx, ry = rectangle.get_xy()\n",
    "cx = rx + rectangle.get_width()/30.\n",
    "cy = ry + rectangle.get_height()/2.\n",
    "\n",
    "text_for_top = 'Top: FFF corner, Wire bonded 2x2, W36-IP7-13 (HPK, split3), offset 24, HV=210V'\n",
    "text_for_mid = 'Middle (Trigger): FFF corner, Bump bonded 16x16, W13 4-5 (FBK), offset 24, HV=210V'\n",
    "text_for_bot = 'Bottom: FFF corner, Bump bonded 16x16, W16 P6 (HPK), offset 6, HV=210V'\n",
    "\n",
    "ax.annotate(f\"{text_for_top} \\n {text_for_mid} \\n {text_for_bot}\", (cx, cy),\n",
    "            color='black', weight='bold', fontsize=11, ha='left', va='center')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "hep.cms.text(loc=0, ax=ax, text=\"Preliminary\", fontsize=25)\n",
    "ax.set_title(rf'TOA$_{{{names[0]}}} \\in \\left[{tdc_cuts[1][2]}, {tdc_cuts[1][3]}\\right]$', loc=\"right\", size=20)\n",
    "ax.set_xlabel('Time Resolution [ps]', fontsize=20)\n",
    "ax.set_yticks(x + width, names)\n",
    "ax.legend(loc='lower right', fontsize=18)\n",
    "ax.set_xlim(30, 100)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
