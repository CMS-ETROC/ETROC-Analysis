{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beamtest_analysis_helper as helper\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from tqdm.notebook import tqdm\n",
    "hep.style.use('CMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_labels = [0, 1, 2, 3]\n",
    "chip_names = [\"ET2_EPIR_Pair1\", \"ET2_BAR_4\", \"ET2_BAR_6\", \"ET2_CNM_1-3\"]\n",
    "high_voltages = [260, 260, 260, 200]\n",
    "\n",
    "# run_names = [\"run31\", \"run32\"]#,\"run33\",\"run34\",\"run35\"]\n",
    "# run_info = \"cover_off_offset15\"\n",
    "# offsets = [15, 15, 15, 15]\n",
    "\n",
    "run_names = [\"run38\", \"run39\"]\n",
    "run_info = \"cover_off_offset10\"\n",
    "offsets = [15, 10, 15, 10]\n",
    "\n",
    "chip_fignames = chip_names\n",
    "chip_figtitles = [\n",
    "    f\"(Trigger) EPIR Pair1 HV{high_voltages[0]}V OS:{offsets[0]}\",\n",
    "    f\"(DUT1) Barcelona 4 HV{high_voltages[1]}V OS:{offsets[1]}\",\n",
    "    f\"(Reference) Barcelona 6 HV{high_voltages[2]}V OS:{offsets[2]}\",\n",
    "    f\"(DUT2) CNM (HPK Sensor) 1-3 HV{high_voltages[3]}V OS:{offsets[3]}\"\n",
    "]\n",
    "\n",
    "board_to_analyze = [0,1,2]\n",
    "ignore_boards = [3]\n",
    "combi_tag = \"dut1\"\n",
    "\n",
    "columns_to_read = ['evt', 'board', 'row', 'col', 'toa', 'tot', 'cal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up bad pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = pd.read_csv(f\"./track_selection/{run_info}_good_track_candidates_track2k_{combi_tag}_first2hours.csv\")\n",
    "track_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pix_df = pd.read_pickle(f'./track_selection/bad_pixels_{run_info}.pkl')\n",
    "bad_pix_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "\n",
    "for idx, irow in bad_pix_df.iterrows():\n",
    "    if irow['board_id'] not in board_to_analyze:\n",
    "        continue\n",
    "\n",
    "    mask = (\n",
    "        (track_df[f\"row_{irow['board_id']}\"] == irow['row']) & (track_df[f\"col_{irow['board_id']}\"] == irow['col'])\n",
    "    )\n",
    "    masks.append(mask)\n",
    "\n",
    "combined_mask = pd.concat(masks, axis=1).any(axis=1)\n",
    "clean_track_df = track_df[~combined_mask].reset_index(drop=True)\n",
    "\n",
    "del masks, combined_mask, track_df, bad_pix_df\n",
    "clean_track_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for rn in run_names:\n",
    "    files += natsorted(glob(f'./desy_TB_analyze/{run_info}/desy_TB_{rn}/*feather'))\n",
    "print(files[:2])\n",
    "print(files[-2:])\n",
    "print('Total:', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "last_evt = 0\n",
    "\n",
    "for idx, ifile in enumerate(tqdm(files)):\n",
    "    run_df = pd.read_feather(ifile, columns=columns_to_read)\n",
    "    run_df = helper.singlehit_event_clear(run_df, ignore_boards=ignore_boards)\n",
    "\n",
    "    run_df['evt'] = run_df['evt'].astype('category').cat.codes.astype('int64')\n",
    "    run_df['evt'] = run_df['evt'].astype('uint64')\n",
    "\n",
    "    if idx > 0:\n",
    "        run_df['evt'] += last_evt\n",
    "\n",
    "    last_evt += run_df['evt'].unique()[-1]\n",
    "\n",
    "    total_df = pd.concat([total_df, run_df])\n",
    "    total_df['evt'] = total_df['evt'].astype('uint64')\n",
    "    del run_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_pivots = {i:[] for i in range(clean_track_df.shape[0])}\n",
    "\n",
    "for itrack in tqdm(range(clean_track_df.shape[0])):\n",
    "\n",
    "    ## Filter only the pixels of interest, dropping other hits on the boards of interest as well as boards not of interest\n",
    "    pix_dict = {}\n",
    "    for idx in board_to_analyze:\n",
    "        pix_dict[idx] = [clean_track_df.iloc[itrack][f'row_{idx}'], clean_track_df.iloc[itrack][f'col_{idx}']]\n",
    "\n",
    "    track_tmp_df = helper.pixel_filter(total_df, pix_dict)\n",
    "\n",
    "    ## Selecting good hits with TDC cuts\n",
    "    tdc_cuts = {}\n",
    "    for idx in board_to_analyze:\n",
    "        # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "        if idx == 0:\n",
    "            tdc_cuts[idx] = [track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]-3, track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]+3,  100, 500, 0, 600]\n",
    "        else:\n",
    "            tdc_cuts[idx] = [track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]-3, track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]+3,  0, 1100, 0, 600]\n",
    "\n",
    "    track_tmp_df = helper.tdc_event_selection(track_tmp_df, tdc_cuts_dict=tdc_cuts)\n",
    "\n",
    "    ## Restrict to events with only one hit on the boards of interest, needed again as we may have dropped hits\n",
    "    track_tmp_df = helper.singlehit_event_clear(track_tmp_df, ignore_boards=ignore_boards)\n",
    "\n",
    "    ## Pivot Table to make tracks\n",
    "    pivot_table = track_tmp_df.pivot(index=[\"evt\"], columns=[\"board\"], values=[\"row\", \"col\", \"toa\", \"tot\", \"cal\"])\n",
    "    track_pivots[itrack].append(pivot_table)\n",
    "    del track_tmp_df, pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save track pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(f'./{run_info}_track_pivot_tables_{combi_tag}')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for key, val in tqdm(track_pivots.items()):\n",
    "    fname = save_dir / f\"{run_info}_track{key}.pkl\"\n",
    "    val[0].to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you don't need a full dataframe since you're using pivot table (Free Memory!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_dict = {}\n",
    "\n",
    "for idx in board_to_analyze:\n",
    "    final_dict[f'row{idx}'] = []\n",
    "    final_dict[f'col{idx}'] = []\n",
    "    final_dict[f'res{idx}'] = []\n",
    "    final_dict[f'err{idx}'] = []\n",
    "\n",
    "for ikey, itable in tqdm(track_pivots.items()):\n",
    "    sum_arr = {}\n",
    "    sum_square_arr = {}\n",
    "    iteration = 100\n",
    "    sampling_fraction = 0.75\n",
    "    counter = 0\n",
    "\n",
    "    for idx in board_to_analyze:\n",
    "        sum_arr[idx] = 0\n",
    "        sum_square_arr[idx] = 0\n",
    "\n",
    "    for iloop in tqdm(range(iteration)):\n",
    "\n",
    "        tdc_filtered_df = itable[0].reset_index()\n",
    "\n",
    "        n = int(sampling_fraction*tdc_filtered_df.shape[0])\n",
    "        indices = np.random.choice(tdc_filtered_df['evt'].unique(), n, replace=False)\n",
    "        tdc_filtered_df = tdc_filtered_df.loc[tdc_filtered_df['evt'].isin(indices)]\n",
    "\n",
    "        if tdc_filtered_df.shape[0] < iteration/(3.*(1-sampling_fraction)):\n",
    "            print('Warning!! Sampling size is too small. Skipping this track')\n",
    "            break\n",
    "\n",
    "        d = {\n",
    "            'evt': tdc_filtered_df['evt'].unique(),\n",
    "        }\n",
    "\n",
    "        for idx in board_to_analyze:\n",
    "            bins = 3.125/tdc_filtered_df['cal'][idx].mean()\n",
    "            d[f'toa_b{str(idx)}'] = 12.5 - tdc_filtered_df['toa'][idx] * bins\n",
    "            d[f'tot_b{str(idx)}'] = (2*tdc_filtered_df['tot'][idx] - np.floor(tdc_filtered_df['tot'][idx]/32)) * bins\n",
    "\n",
    "        df_in_time = pd.DataFrame(data=d)\n",
    "        del d, tdc_filtered_df\n",
    "\n",
    "        if(len(board_to_analyze)==3):\n",
    "            corr_toas = helper.three_board_iterative_timewalk_correction(df_in_time, 5, 3, board_list=board_to_analyze)\n",
    "        elif(len(board_to_analyze)==4):\n",
    "            corr_toas = helper.four_board_iterative_timewalk_correction(df_in_time, 5, 3)\n",
    "        else:\n",
    "            print(\"You have less than 3 boards to analyze\")\n",
    "            break\n",
    "\n",
    "        diffs = {}\n",
    "        for board_a in board_to_analyze:\n",
    "            for board_b in board_to_analyze:\n",
    "                if board_b <= board_a:\n",
    "                    continue\n",
    "                name = f\"{board_a}{board_b}\"\n",
    "                diffs[name] = np.asarray(corr_toas[f'toa_b{board_a}'] - corr_toas[f'toa_b{board_b}'])\n",
    "        hists = {}\n",
    "        for key in diffs.keys():\n",
    "            hists[key] = hist.Hist(hist.axis.Regular(80, -1.2, 1.2, name=\"TWC_delta_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "            hists[key].fill(diffs[key])\n",
    "\n",
    "        try:\n",
    "            fit_params_lmfit = {}\n",
    "            for key in hists.keys():\n",
    "                params = helper.lmfit_gaussfit_with_pulls(diffs[key], hists[key], std_range_cut=0.4, width_factor=1.25, fig_title='',\n",
    "                                                    use_pred_uncert=True, no_show_fit=False, no_draw=True, get_chisqure=False)\n",
    "                fit_params_lmfit[key] = params\n",
    "            del params, hists, diffs, corr_toas\n",
    "\n",
    "            if(len(board_to_analyze)==3):\n",
    "                resolutions = helper.return_resolution_three_board(fit_params_lmfit, var=list(fit_params_lmfit.keys()), board_list=board_to_analyze)\n",
    "            elif(len(board_to_analyze)==4):\n",
    "                resolutions = helper.return_resolution_four_board(fit_params_lmfit)\n",
    "            else:\n",
    "                print(\"You have less than 3 boards to analyze\")\n",
    "                break\n",
    "\n",
    "            if any(np.isnan(val) for key, val in resolutions.items()):\n",
    "                print('fit results is not good, skipping this iteration')\n",
    "                continue\n",
    "\n",
    "            for key in resolutions.keys():\n",
    "                sum_arr[key] += resolutions[key]\n",
    "                sum_square_arr[key] += resolutions[key]**2\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        except:\n",
    "            print('Failed, skipping')\n",
    "            del hists, diffs, corr_toas\n",
    "\n",
    "    if counter != 0:\n",
    "        for idx in board_to_analyze:\n",
    "            final_dict[f'row{idx}'].append(itable[0]['row'][idx].unique()[0])\n",
    "            final_dict[f'col{idx}'].append(itable[0]['col'][idx].unique()[0])\n",
    "\n",
    "        for key in sum_arr.keys():\n",
    "            mean = sum_arr[key]/counter\n",
    "            std = np.sqrt((1/(counter-1))*(sum_square_arr[key]-counter*(mean**2)))\n",
    "            final_dict[f'res{key}'].append(mean)\n",
    "            final_dict[f'err{key}'].append(std)\n",
    "    else:\n",
    "        print('Track is not validate for bootstrapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_tag = \"_trigTOA100_500_oneHitTrigDut1Ref_track2k_first2hours\"\n",
    "# csv_tag = \"_trigTOA100_500_oneHitTrigDut2Ref_track2k_first2hours\"\n",
    "# csv_tag = \"_trigTOA100_500_oneHitTrigDut1Dut2Ref_track2k_first2hours\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(f'./{run_info}{csv_tag}_resolutions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
