{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import beamtest_analysis_helper as helper\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "from collections import defaultdict\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from tqdm.notebook import tqdm\n",
    "hep.style.use('CMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_labels = [0, 1, 2, 3]\n",
    "chip_names = [\"ET2_EPIR_Pair1\", \"ET2_BAR_4\", \"ET2_BAR_6\", \"ET2_CNM_1-3\"]\n",
    "high_voltages = [260, 260, 260, 200]\n",
    "\n",
    "run_names = [\"run31\", \"run32\"]\n",
    "run_info = \"cover_off_offset15\"\n",
    "offsets = [15, 15, 15, 15]\n",
    "\n",
    "# run_names = [\"run38\", \"run39\"]\n",
    "# run_info = \"cover_off_offset10\"\n",
    "# offsets = [15, 10, 15, 10]\n",
    "\n",
    "chip_fignames = chip_names\n",
    "chip_figtitles = [\n",
    "    f\"(Trigger) EPIR Pair1 HV{high_voltages[0]}V OS:{offsets[0]}\",\n",
    "    f\"(DUT1) Barcelona 4 HV{high_voltages[1]}V OS:{offsets[1]}\",\n",
    "    f\"(Reference) Barcelona 6 HV{high_voltages[2]}V OS:{offsets[2]}\",\n",
    "    f\"(DUT2) CNM (HPK Sensor) 1-3 HV{high_voltages[3]}V OS:{offsets[3]}\"\n",
    "]\n",
    "\n",
    "board_to_analyze = [0,1,2]\n",
    "ignore_boards = [3]\n",
    "combi_tag = \"dut1\"\n",
    "toa_cut = [100, 500]\n",
    "\n",
    "columns_to_read = ['evt', 'board', 'row', 'col', 'toa', 'tot', 'cal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up bad pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = pd.read_csv(f\"./track_selection/{run_info}_good_track_candidates_track2k_{combi_tag}_first2hours.csv\")\n",
    "track_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_pix_df = pd.read_pickle(f'./track_selection/bad_pixels_{run_info}.pkl')\n",
    "bad_pix_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "\n",
    "for idx, irow in bad_pix_df.iterrows():\n",
    "    if irow['board_id'] not in board_to_analyze:\n",
    "        continue\n",
    "\n",
    "    mask = (\n",
    "        (track_df[f\"row_{irow['board_id']}\"] == irow['row']) & (track_df[f\"col_{irow['board_id']}\"] == irow['col'])\n",
    "    )\n",
    "    masks.append(mask)\n",
    "\n",
    "combined_mask = pd.concat(masks, axis=1).any(axis=1)\n",
    "clean_track_df = track_df[~combined_mask].reset_index(drop=True)\n",
    "\n",
    "del masks, combined_mask, track_df, bad_pix_df\n",
    "clean_track_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for rn in run_names:\n",
    "    files += natsorted(glob(f'./desy_TB_analyze/{run_info}/desy_TB_{rn}/*feather'))\n",
    "print(files[:2])\n",
    "print(files[-2:])\n",
    "print('Total:', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "last_evt = 0\n",
    "\n",
    "for idx, ifile in enumerate(tqdm(files)):\n",
    "    run_df = pd.read_feather(ifile, columns=columns_to_read)\n",
    "    run_df = helper.singlehit_event_clear(run_df, ignore_boards=ignore_boards)\n",
    "\n",
    "    run_df['evt'] = run_df['evt'].astype('category').cat.codes.astype('int64')\n",
    "    run_df['evt'] = run_df['evt'].astype('uint64')\n",
    "\n",
    "    if idx > 0:\n",
    "        run_df['evt'] += last_evt\n",
    "\n",
    "    last_evt += run_df['evt'].unique()[-1]\n",
    "\n",
    "    total_df = pd.concat([total_df, run_df])\n",
    "    total_df['evt'] = total_df['evt'].astype('uint64')\n",
    "    del run_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find parameters for diagonal cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(total_df,\n",
    "                     bad_pix_df[bad_pix_df['board_id'].isin(board_to_analyze)][['board_id', 'row', 'col']],\n",
    "                     how='left',\n",
    "                     left_on=['board', 'row', 'col'],\n",
    "                     right_on=['board_id', 'row', 'col'],\n",
    "                     indicator=True)\n",
    "\n",
    "clean_hit_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['board_id', '_merge']).reset_index(drop=True)\n",
    "del merged_df\n",
    "\n",
    "event_board_counts = clean_hit_df.groupby(['evt', 'board']).size().unstack(fill_value=0)\n",
    "event_selection_col = None\n",
    "\n",
    "trig_selection = (event_board_counts[0] == 1)\n",
    "ref_selection = (event_board_counts[2] == 1)\n",
    "event_selection_col = trig_selection & ref_selection\n",
    "\n",
    "sub_clean_hit_df = clean_hit_df[clean_hit_df['evt'].isin(event_board_counts[event_selection_col].index)]\n",
    "sub_clean_hit_df.reset_index(inplace=True, drop=True)\n",
    "del clean_hit_df, event_board_counts, event_selection_col, trig_selection, ref_selection\n",
    "\n",
    "## Selecting good hits\n",
    "tdc_cuts = {}\n",
    "for idx in board_to_analyze:\n",
    "    # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "    if idx == 0:\n",
    "        tdc_cuts[idx] = [sub_clean_hit_df.loc[sub_clean_hit_df['board'] == idx]['cal'].mode()[0]-50, sub_clean_hit_df.loc[sub_clean_hit_df['board'] == idx]['cal'].mode()[0]+50,  100, 500, 0, 600]\n",
    "    else:\n",
    "        tdc_cuts[idx] = [sub_clean_hit_df.loc[sub_clean_hit_df['board'] == idx]['cal'].mode()[0]-50, sub_clean_hit_df.loc[sub_clean_hit_df['board'] == idx]['cal'].mode()[0]+50,  0, 1100, 0, 600]\n",
    "\n",
    "filtered_df = helper.tdc_event_selection(sub_clean_hit_df, tdc_cuts_dict=tdc_cuts)\n",
    "del sub_clean_hit_df\n",
    "\n",
    "params = np.polyfit(filtered_df.loc[filtered_df['board'] == 0]['toa'].reset_index(drop=True), filtered_df.loc[filtered_df['board'] == 2]['toa'].reset_index(drop=True), 1)\n",
    "del filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_pivots = defaultdict(list)\n",
    "\n",
    "for itrack in tqdm(range(clean_track_df.shape[0])):\n",
    "\n",
    "    ## Filter only the pixels of interest, dropping other hits on the boards of interest as well as boards not of interest\n",
    "    pix_dict = {}\n",
    "    for idx in board_to_analyze:\n",
    "        pix_dict[idx] = [clean_track_df.iloc[itrack][f'row_{idx}'], clean_track_df.iloc[itrack][f'col_{idx}']]\n",
    "\n",
    "    track_tmp_df = helper.pixel_filter(total_df, pix_dict)\n",
    "\n",
    "    ## Selecting good hits with TDC cuts\n",
    "    tdc_cuts = {}\n",
    "    for idx in board_to_analyze:\n",
    "        # board ID: [CAL LB, CAL UB, TOA LB, TOA UB, TOT LB, TOT UB]\n",
    "        if idx == 0:\n",
    "            tdc_cuts[idx] = [track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]-3, track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]+3,  toa_cut[0], toa_cut[1], 0, 600]\n",
    "        else:\n",
    "            tdc_cuts[idx] = [track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]-3, track_tmp_df.loc[track_tmp_df['board'] == idx]['cal'].mode()[0]+3,  0, 1100, 0, 600]\n",
    "\n",
    "    track_tmp_df = helper.tdc_event_selection(track_tmp_df, tdc_cuts_dict=tdc_cuts)\n",
    "\n",
    "    x = track_tmp_df.loc[track_tmp_df['board'] == 0]['toa'].reset_index(drop=True)\n",
    "    y = track_tmp_df.loc[track_tmp_df['board'] == 2]['toa'].reset_index(drop=True)\n",
    "    distance = (x*params[0] - y + params[1])/(np.sqrt(params[0]**2 + 1))\n",
    "    evts = track_tmp_df.loc[track_tmp_df['board'] == 0].reset_index(drop=True)[distance < 3.*np.std(distance)]['evt'].unique()\n",
    "\n",
    "    track_tmp_df = track_tmp_df.loc[track_tmp_df['evt'].isin(evts)]\n",
    "\n",
    "    ## Pivot Table to make tracks\n",
    "    pivot_table = track_tmp_df.pivot(index=[\"evt\"], columns=[\"board\"], values=[\"row\", \"col\", \"toa\", \"tot\", \"cal\"])\n",
    "    track_pivots[itrack].append(pivot_table)\n",
    "    del track_tmp_df, pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save track pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path(f'./{run_info}_track_pivot_tables_{combi_tag}_TOA{toa_cut[0]}to{toa_cut[1]}_TOA_CorrelationCut')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for key, val in tqdm(track_pivots.items()):\n",
    "    r0 = clean_track_df.iloc[key][f'row_{board_to_analyze[0]}']\n",
    "    c0 = clean_track_df.iloc[key][f'col_{board_to_analyze[0]}']\n",
    "    r1 = clean_track_df.iloc[key][f'row_{board_to_analyze[1]}']\n",
    "    c1 = clean_track_df.iloc[key][f'col_{board_to_analyze[1]}']\n",
    "    r2 = clean_track_df.iloc[key][f'row_{board_to_analyze[2]}']\n",
    "    c2 = clean_track_df.iloc[key][f'col_{board_to_analyze[2]}']\n",
    "    fname = save_dir / f\"track_{chip_names[board_to_analyze[0]]}_R{r0}C{c0}_{chip_names[board_to_analyze[1]]}_R{r1}C{c1}_{chip_names[board_to_analyze[2]]}_R{r2}C{c2}.pkl\"\n",
    "    val[0].to_pickle(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you don't need a full dataframe since you're using pivot table (Free Memory!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = defaultdict(list)\n",
    "\n",
    "for ikey, itable in tqdm(track_pivots.items()):\n",
    "    sum_arr = defaultdict(float)\n",
    "    sum_square_arr = defaultdict(float)\n",
    "    iteration = 100\n",
    "    sampling_fraction = 0.75\n",
    "    counter = 0\n",
    "\n",
    "    for iloop in tqdm(range(iteration)):\n",
    "\n",
    "        tdc_filtered_df = itable[0].reset_index()\n",
    "\n",
    "        n = int(sampling_fraction*tdc_filtered_df.shape[0])\n",
    "        indices = np.random.choice(tdc_filtered_df['evt'].unique(), n, replace=False)\n",
    "        tdc_filtered_df = tdc_filtered_df.loc[tdc_filtered_df['evt'].isin(indices)]\n",
    "\n",
    "        if tdc_filtered_df.shape[0] < iteration/(3.*(1-sampling_fraction)):\n",
    "            print('Warning!! Sampling size is too small. Skipping this track')\n",
    "            break\n",
    "\n",
    "        d = {\n",
    "            'evt': tdc_filtered_df['evt'].unique(),\n",
    "        }\n",
    "\n",
    "        for idx in board_to_analyze:\n",
    "            bins = 3.125/tdc_filtered_df['cal'][idx].mean()\n",
    "            d[f'toa_b{str(idx)}'] = 12.5 - tdc_filtered_df['toa'][idx] * bins\n",
    "            d[f'tot_b{str(idx)}'] = (2*tdc_filtered_df['tot'][idx] - np.floor(tdc_filtered_df['tot'][idx]/32)) * bins\n",
    "\n",
    "        df_in_time = pd.DataFrame(data=d)\n",
    "        del d, tdc_filtered_df\n",
    "\n",
    "        if(len(board_to_analyze)==3):\n",
    "            corr_toas = helper.three_board_iterative_timewalk_correction(df_in_time, 5, 3, board_list=board_to_analyze)\n",
    "        elif(len(board_to_analyze)==4):\n",
    "            corr_toas = helper.four_board_iterative_timewalk_correction(df_in_time, 5, 3)\n",
    "        else:\n",
    "            print(\"You have less than 3 boards to analyze\")\n",
    "            break\n",
    "\n",
    "        diffs = {}\n",
    "        for board_a in board_to_analyze:\n",
    "            for board_b in board_to_analyze:\n",
    "                if board_b <= board_a:\n",
    "                    continue\n",
    "                name = f\"{board_a}{board_b}\"\n",
    "                diffs[name] = np.asarray(corr_toas[f'toa_b{board_a}'] - corr_toas[f'toa_b{board_b}'])\n",
    "        hists = {}\n",
    "        for key in diffs.keys():\n",
    "            hists[key] = hist.Hist(hist.axis.Regular(80, -1.2, 1.2, name=\"TWC_delta_TOA\", label=r'Time Walk Corrected $\\Delta$TOA [ns]'))\n",
    "            hists[key].fill(diffs[key])\n",
    "\n",
    "        try:\n",
    "            fit_params_lmfit = {}\n",
    "            for key in hists.keys():\n",
    "                params = helper.lmfit_gaussfit_with_pulls(diffs[key], hists[key], std_range_cut=0.4, width_factor=1.25, fig_title='',\n",
    "                                                    chipNames='', use_pred_uncert=True, no_show_fit=False, no_draw=True, get_chisqure=False)\n",
    "                fit_params_lmfit[key] = params\n",
    "            del params, hists, diffs, corr_toas\n",
    "\n",
    "            if(len(board_to_analyze)==3):\n",
    "                resolutions = helper.return_resolution_three_board(fit_params_lmfit, var=list(fit_params_lmfit.keys()), board_list=board_to_analyze)\n",
    "            elif(len(board_to_analyze)==4):\n",
    "                resolutions = helper.return_resolution_four_board(fit_params_lmfit)\n",
    "            else:\n",
    "                print(\"You have less than 3 boards to analyze\")\n",
    "                break\n",
    "\n",
    "            if any(np.isnan(val) for key, val in resolutions.items()):\n",
    "                print('fit results is not good, skipping this iteration')\n",
    "                continue\n",
    "\n",
    "            for key in resolutions.keys():\n",
    "                sum_arr[key] += resolutions[key]\n",
    "                sum_square_arr[key] += resolutions[key]**2\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "            del hists, diffs, corr_toas\n",
    "\n",
    "    if counter != 0:\n",
    "        for idx in board_to_analyze:\n",
    "            final_dict[f'row{idx}'].append(itable[0]['row'][idx].unique()[0])\n",
    "            final_dict[f'col{idx}'].append(itable[0]['col'][idx].unique()[0])\n",
    "\n",
    "        for key in sum_arr.keys():\n",
    "            mean = sum_arr[key]/counter\n",
    "            std = np.sqrt((1/(counter-1))*(sum_square_arr[key]-counter*(mean**2)))\n",
    "            final_dict[f'res{key}'].append(mean)\n",
    "            final_dict[f'err{key}'].append(std)\n",
    "    else:\n",
    "        print('Track is not validate for bootstrapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_tag = \"_trigTOA100_500_oneHitTrigDut1Ref_track2k_first2hours\"\n",
    "# csv_tag = \"_trigTOA100_500_oneHitTrigDut2Ref_track2k_first2hours\"\n",
    "# csv_tag = \"_trigTOA100_500_oneHitTrigDut1Dut2Ref_track2k_first2hours\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(f'./{run_info}{csv_tag}_resolutions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "packages",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
